{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model is an OLS model with feature selction being done through a filter method (univariate).Our results indicate that minimum temperature, precipitation, and number of intersections are the most important features to predict air quality.\n",
    "\n",
    "The code chunks below consists of all other models we developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import basic python packages for data analysis and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.lines as mlines\n",
    "import pylab as plot\n",
    "import matplotlib\n",
    "import random\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import math\n",
    "import time\n",
    "\n",
    "### Import Scipy stats packages\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# import sklearn packages\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(style = 'whitegrid')\n",
    "sns.set_palette('bright')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all datasets to create master_df\n",
    "\n",
    "root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "df_aq = pd.read_csv(root + \"/data/cleaned/air_quality_NO2.csv\", index_col=0)[['value','latitude', 'longitude']]\n",
    "df_met = pd.read_csv(root + \"/data/cleaned/nO2_met.csv\", index_col=0)\n",
    "df_fac = pd.read_csv(root + \"/data/cleaned/no2_fac_data.csv\", index_col=0)\n",
    "# df_fac.drop(df_fac.columns[df_fac.columns.str.contains('_emsdist')], axis=1, inplace=True)\n",
    "df_traffic = pd.read_csv(root + \"/data/cleaned/intersection_final.csv\", index_col=0)\n",
    "\n",
    "df_m1 = df_aq.merge(df_met, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_m2 = df_m1.merge(df_fac, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_merged = df_m2.merge(df_traffic, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_merged.drop(columns = ['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "X = df_merged.drop(\"value\",1) \n",
    "y = df_merged[\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Modeling - Using Cross Validation and Randomized/Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB -- Code to get list of params for an estimator:\n",
    "#regressor.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scale and transform input data\n",
    "def get_data(X, y):\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    # scaling the data\n",
    "    feature_scaler = StandardScaler()\n",
    "    X = feature_scaler.fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "# acquiring transformed data\n",
    "X_arr, y_arr = get_data(X, y)\n",
    "cols = np.array(X.columns)\n",
    "\n",
    "# master list for all model scores\n",
    "model_scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_tuning(X_arr, y_arr, grid_param):\n",
    "    print(\"Tuning model\")\n",
    "    # defining the regressor\n",
    "    regressor = RandomForestRegressor(random_state=0)\n",
    "\n",
    "    # doing randomized search\n",
    "    rd_sr = RandomizedSearchCV(estimator=regressor,\n",
    "                               param_distributions=grid_param,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               cv=10,\n",
    "                               n_jobs=-1)\n",
    "\n",
    "    rd_sr.fit(X_arr, y_arr)\n",
    "\n",
    "    # best estimator and it's components\n",
    "    best_parameters = rd_sr.best_params_\n",
    "    best_est_score = rd_sr.best_score_\n",
    "    feature_imp = rd_sr.best_estimator_.feature_importances_\n",
    "    \n",
    "    return rd_sr, best_parameters, best_est_score, feature_imp\n",
    "    \n",
    "\n",
    "def rf_feat_selection(X_arr, y_arr, grid_param, cols):\n",
    "    rd_sr, best_parameters, best_est_score, feature_imp = rf_tuning(X_arr, y_arr, grid_param)\n",
    "    \n",
    "    print(\"Feature selection\")\n",
    "    # plotting feat imp histogram\n",
    "    pd.Series(feature_imp, index=cols).sort_values().plot(kind='barh')\n",
    "    \n",
    "    # calculating optimum number of features\n",
    "    high_score = float(-np.inf)\n",
    "    \n",
    "    for thresh in np.arange(min(feature_imp) + 0.002, max(feature_imp), 0.002):\n",
    "        # identify the most important features using threshold (use RFECV or top 10 percentile later)\n",
    "        sfm = SelectFromModel(rd_sr.best_estimator_, threshold=thresh, prefit=True)\n",
    "        selected_features = cols[sfm.get_support(indices=True)]\n",
    "        print(\"Number of selected features:\", len(selected_features))\n",
    "\n",
    "        # updated dataset\n",
    "        X_updated = sfm.transform(X_arr)\n",
    "\n",
    "        # train a second regressor on this new dataset with hyperparameters of best estimators above using CV\n",
    "        cv_score_list = cross_val_score(RandomForestRegressor(**best_parameters), X_updated, \n",
    "                                        y_arr, cv=10, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        if cv_score_list.mean() > high_score and len(selected_features) > 10:\n",
    "            high_score = cv_score_list.mean()\n",
    "            final_feat_lst = selected_features\n",
    "            feat_arr = X_updated\n",
    "            \n",
    "    return high_score, final_feat_lst, feat_arr, best_parameters, best_est_score\n",
    "\n",
    "\n",
    "def rf_model(X_arr, y_arr, grid_param, cols):\n",
    "    high_score, final_feat_lst, feat_arr, best_parameters, \\\n",
    "        best_est_score = rf_feat_selection(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "    print(\"Saving model to file\")\n",
    "    # saving the model to file\n",
    "    final_model = RandomForestRegressor(**best_parameters)\n",
    "    final_model.fit(feat_arr, y_arr)\n",
    "    pkl_filename = \"rf_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "\n",
    "    print(\"MSE before feature selection:\", best_est_score)\n",
    "    print(\"MSE after feature selection:\", high_score)\n",
    "    print(\"The final list of features are:\", final_feat_lst)\n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    \n",
    "    return high_score\n",
    "\n",
    "\n",
    "grid_param = {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [10, 20, 30, 40, 50],\n",
    "        #'min_samples_split': [2, 5, 10, 15, 20],\n",
    "        #'min_samples_leaf': [1, 2, 5, 10, 15],\n",
    "        #'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "\n",
    "# building the model\n",
    "final_score = rf_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Random Forest\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def xgb_tuning(X_arr, y_arr, grid_param):\n",
    "    print(\"Tuning XGB model\")\n",
    "    \n",
    "    # defining the regressor\n",
    "    \n",
    "    regressor = XGBRegressor(random_state=0)\n",
    "    \n",
    "    # RandomizedSearch\n",
    "    rd_sr = RandomizedSearchCV(estimator = regressor,\n",
    "                                   param_distributions = grid_param,\n",
    "                                   scoring = 'neg_mean_squared_error',\n",
    "                                   cv = 10,\n",
    "                                   n_jobs = -1\n",
    "                                  )\n",
    "    \n",
    "    rd_sr.fit(X_arr, y_arr)\n",
    "    \n",
    "    # best estimator and its components\n",
    "    best_parameters = rd_sr.best_params_\n",
    "    best_est_score = rd_sr.best_score_\n",
    "    feature_imp = rd_sr.best_estimator_.feature_importances_\n",
    "\n",
    "    return rd_sr, best_parameters, best_est_score, feature_imp\n",
    "\n",
    "def plot_hist(data):\n",
    "    plt.figure(figsize = (20,8))\n",
    "    ax = sns.barplot(y = 'Importance', x = 'Feature', \n",
    "                     hue = 'grouping',  \n",
    "                     data = data[:30],\n",
    "                     dodge = False, palette = 'muted')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.xlabel(\"Feature\", size = 20)\n",
    "    plt.xticks(size = 20)\n",
    "    plt.yticks(size = 16)\n",
    "    plt.ylabel(\"Importance\", size = 20)\n",
    "    plt.title(\"Feature selection using XGBoost and Cross Validation\", size = 20)\n",
    "    plt.show()\n",
    "\n",
    "def add_group(data):\n",
    "    data['grouping'] = \" \"\n",
    "    for index, string in enumerate(data['Feature']):\n",
    "        group = re.findall(r'-(.*?)-', string)\n",
    "        if group:\n",
    "            data.loc[index, 'grouping'] = group[0]\n",
    "        else:\n",
    "            data.loc[index, 'grouping'] = data.loc[index, 'Feature']\n",
    "    \n",
    "def xgb_feat_selection(X_arr, y_arr, grid_param, cols):\n",
    "    \n",
    "    rd_sr, best_parameters, best_est_score, feature_imp = xgb_tuning(\n",
    "        X_arr, y_arr, grid_param)\n",
    "    \n",
    "    print(\"Feature selection\")\n",
    "    # plotting feat imp histogram\n",
    "    sf = pd.Series(feature_imp, index=cols)\n",
    "    df = pd.DataFrame({'Feature':sf.index, 'Importance':sf.values})\n",
    "    df = df.sort_values('Importance', ascending = False).reset_index()\n",
    "    add_group(df)\n",
    "    plot_hist(df)\n",
    "    \n",
    "    # calculating optimum number of features\n",
    "    neg_score = float(-np.inf)\n",
    "    \n",
    "    for thresh in np.arange(min(feature_imp) + 0.002, max(feature_imp), 0.002):\n",
    "        # identify the most important features using threshold (use RFECV or top 10 percentile later)\n",
    "        sfm = SelectFromModel(rd_sr.best_estimator_, threshold=thresh, prefit=True)\n",
    "        selected_features = cols[sfm.get_support(indices=True)]\n",
    "        print(\"Number of selected features:\", len(selected_features))\n",
    "\n",
    "        # updated dataset\n",
    "        X_updated = sfm.transform(X_arr)\n",
    "\n",
    "        # train a second regressor on this new dataset with hyperparameters of best estimators above using CV\n",
    "        cv_score_list = cross_val_score(XGBRegressor(**best_parameters), X_updated, \n",
    "                                        y_arr, cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        if cv_score_list.mean() > neg_score and len(selected_features) > 10:\n",
    "            high_score = cv_score_list.mean()\n",
    "            final_feat_lst = selected_features\n",
    "            feat_arr = X_updated\n",
    "            \n",
    "    return high_score, final_feat_lst, feat_arr, best_parameters, best_est_score\n",
    "\n",
    "def xgb_model(X_arr, y_arr, grid_param, cols):\n",
    "    high_score, final_feat_lst, feat_arr, best_parameters, \\\n",
    "        best_est_score = xgb_feat_selection(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "    print(\"Saving model to file\")\n",
    "    # saving the model to file\n",
    "    final_model = XGBRegressor(**best_parameters)\n",
    "    final_model.fit(feat_arr, y_arr)\n",
    "    pkl_filename = \"xgb_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "\n",
    "    print(\"MSE before feature selection:\", best_est_score)\n",
    "    print(\"MSE after feature selection:\", high_score)\n",
    "    print(\"The final list of features are:\", final_feat_lst)\n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    \n",
    "    return high_score\n",
    "\n",
    "grid_param = {\n",
    "        'n_estimators': range(50, 500, 50),\n",
    "        'max_depth': range(5, 50, 5)\n",
    "    }\n",
    "\n",
    "\n",
    "# building the model\n",
    "final_score = xgb_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Extreme Gradient Boosting\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_model(X_arr, y_arr, grid_param, cols):\n",
    "    # defining the regressor\n",
    "    regressor = Lasso(random_state=1)\n",
    "\n",
    "    # do GridSearchCV\n",
    "    gd_sr = GridSearchCV(estimator=regressor,\n",
    "                        param_grid=grid_param,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=10,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "    gd_sr.fit(X_arr, y_arr)\n",
    "    best_parameters = gd_sr.best_params_\n",
    "    best_score = gd_sr.best_score_\n",
    "    \n",
    "    # feature importance list\n",
    "    coef = pd.Series(gd_sr.best_estimator_.coef_, index = cols)\n",
    "    print(\"Lasso picked \" + str(sum(coef != 0)) + \n",
    "      \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "    \n",
    "    # plotting the selected features\n",
    "    feat_imp = coef[coef != 0].sort_values()\n",
    "    plt.figure(figsize = (10, 15))\n",
    "    feat_imp.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature Importance using Lasso\")\n",
    "    plt.yticks(fontsize = 10)\n",
    "    plt.show()\n",
    "    \n",
    "    # saving the model to file\n",
    "    pkl_filename = \"lasso_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(gd_sr.best_estimator_, file)\n",
    "    \n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    print(\"The Neg. MSE after tuning is:\", best_score)\n",
    "    \n",
    "    return best_score\n",
    "\n",
    "\n",
    "grid_param = {\n",
    "            'alpha':[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "            }\n",
    "\n",
    "# building the model\n",
    "final_score = lasso_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Lasso\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_model(X_arr, y_arr, grid_param, cols):\n",
    "    # defining the regressor\n",
    "    regressor = Ridge(random_state=1)\n",
    "\n",
    "    # do GridSearchCV\n",
    "    gd_sr = GridSearchCV(estimator=regressor,\n",
    "                        param_grid=grid_param,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=10,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "    gd_sr.fit(X_arr, y_arr)\n",
    "    best_parameters = gd_sr.best_params_\n",
    "    best_score = gd_sr.best_score_\n",
    "    \n",
    "    # feature importance list\n",
    "    coef = pd.Series(gd_sr.best_estimator_.coef_, index = cols)\n",
    "    print(\"Ridge picked \" + str(sum(coef != 0)) + \n",
    "      \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "    \n",
    "    # plotting the selected features\n",
    "    feat_imp = coef[coef != 0].sort_values()\n",
    "    plt.figure(figsize = (10, 25))\n",
    "    feat_imp.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature Importance using Ridge\")\n",
    "    plt.yticks(fontsize = 10)\n",
    "    plt.show()\n",
    "    \n",
    "    # saving the model to file\n",
    "    pkl_filename = \"ridge_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(gd_sr.best_estimator_, file)\n",
    "    \n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    print(\"The Neg. MSE after tuning is:\", best_score)\n",
    "    \n",
    "    return best_score\n",
    "\n",
    "\n",
    "grid_param = {\n",
    "            'alpha':[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "            'solver':['auto', 'svd', 'cholesky', 'sag'] \n",
    "            }\n",
    "\n",
    "# building the model\n",
    "final_score = ridge_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Ridge\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Linear regression -- With and without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X_arr, y_arr, cols, pca_flag=False):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_arr, y_arr, \n",
    "                                                    test_size=0.3)\n",
    "    if pca_flag:\n",
    "        pca = PCA(.95)\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "        \n",
    "        # saving model to file\n",
    "        pkl_filename = \"pca.pkl\"\n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(pca, file)\n",
    "        \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = - mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # saving regression model to file\n",
    "    pkl_filename = \"linear_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    print(\"Neg. MSE is:\", score)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# building the linear model without PCA\n",
    "final_score = linear_model(X_arr, y_arr, cols)\n",
    "model_scores[\"Linear\"] = final_score\n",
    "\n",
    "# building the linear model with PCA feature selection\n",
    "final_score = linear_model(X_arr, y_arr, cols, pca_flag=True)\n",
    "model_scores[\"Linear(PCA)\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) Model evaluation / Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_scores_df = pd.DataFrame.from_dict(model_scores, orient = 'index')\n",
    "model_scores_df.columns = [\"Neg. Mean Squared Error\"]\n",
    "plt.figure(figsize = (10, 7))\n",
    "ax = model_scores_df[\"Neg. Mean Squared Error\"].sort_values(ascending=False).plot(kind = \"barh\", title = \"Comparing Model Performance\")\n",
    "fig = ax.get_figure()\n",
    "axes = plt.gca()\n",
    "plt.yticks(fontsize = 10)\n",
    "#plt.show()\n",
    "fig.savefig('model_comparison.jpg', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D) Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually creating model_scores dict\n",
    "model_scores = {}\n",
    "model_scores[\"Random Forest\"] = -3.554107593003395e-08\n",
    "model_scores[\"Extreme Gradient Boosting\"] = -8.322085370215042e-08\n",
    "model_scores[\"Lasso\"] = -2.65918672898003e-07\n",
    "model_scores[\"Ridge\"] = -1.8350045158023289e-07\n",
    "model_scores[\"Linear\"] = -1.7429402374949595e-07\n",
    "model_scores[\"Linear(PCA)\"] = -3.013181658827342e-07\n",
    "#model_scores[\"Neural Regression\"] = -4.3356190018424304e-07  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X_arr, y_arr, cols):\n",
    "    \n",
    "    ## determining optimum no of features\n",
    "    \n",
    "    nof_list=np.arange(1,len(cols))            \n",
    "    neg_score=float(np.inf)\n",
    "    nof=0           \n",
    "    \n",
    "    for n in range(len(nof_list)):\n",
    "        model = LinearRegression()\n",
    "        rfe = RFE(model,nof_list[n])\n",
    "        X_rfe = rfe.fit_transform(X_arr,y_arr)\n",
    "        cv_score_list = cross_val_score(model, X_rfe, \n",
    "                                        y_arr, cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        if(cv_score_list.mean() < neg_score):\n",
    "            neg_score = cv_score_list.mean()\n",
    "            nof = nof_list[n]\n",
    "            \n",
    "    ## getting list of optimum features\n",
    "    \n",
    "    model = LinearRegression()\n",
    "\n",
    "    # initializing RFE model\n",
    "    rfe = RFE(model, nof)    \n",
    "\n",
    "    # transforming data using RFE\n",
    "    X_rfe = rfe.fit_transform(X_arr,y_arr)\n",
    "\n",
    "    # fitting the data to model\n",
    "    model.fit(X_rfe, y_arr)\n",
    "    \n",
    "    # saving model to file\n",
    "    pkl_filename = \"linear_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    temp = pd.Series(rfe.support_, index = cols)\n",
    "    selected_features_rfe = temp[temp==True].index\n",
    "\n",
    "    print(\"Optimum number of features: %d\" %nof)\n",
    "    print(\"Selected features:\", selected_features_rfe)\n",
    "    print(\"Score with %d features: %f\" % (nof, neg_score))\n",
    "    \n",
    "    return neg_score\n",
    "\n",
    "\n",
    "# building the model\n",
    "final_score = linear_model(X_arr, y_arr, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Linear RFE\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection via Filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = df_merged.corr()\n",
    "sns.heatmap(cor, annot=False, cmap=plt.cm.Reds)\n",
    "plt.show()\n",
    "\n",
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"value\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[(cor_target>0.2) & (cor_target != 1.0)]  # anything above 0.2 gives nothing\n",
    "relevant_features\n",
    "\n",
    "# removing features with high levels of multicollinearity\n",
    "selected_features_corr = df_merged[list(relevant_features.index)].corr()\n",
    "\n",
    "# identifying all the features that have a correlation higher than 0.90 or lower than -0.90 indicating a strong positive or negative correlation\n",
    "\n",
    "threshold_1 = 0.90\n",
    "threshold_2 = -0.90\n",
    "\n",
    "def features_high_corr(df_features_corr):\n",
    "    columns = np.full((df_features_corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(df_features_corr.shape[0]):\n",
    "        for j in range(i+1, df_features_corr.shape[0]):\n",
    "            if (df_features_corr.iloc[i,j] >= threshold_1) | (df_features_corr.iloc[i,j] <= threshold_2) :\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    selected_columns = df_features_corr.columns[columns]\n",
    "    return selected_columns\n",
    "\n",
    "# list of features that are not highly correlated\n",
    "selected_features = features_high_corr(selected_features_corr)\n",
    "print(\"Features in dataset that are not highly correlated: \")\n",
    "print(selected_features)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
