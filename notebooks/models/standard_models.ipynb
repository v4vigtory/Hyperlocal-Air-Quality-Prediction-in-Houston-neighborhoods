{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model is an OLS model with feature selction being done through a filter method (univariate).Our results indicate that minimum temperature, precipitation, and number of intersections are the most important features to predict air quality.\n",
    "\n",
    "The code chunks below consists of all other models we developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import basic python packages for data analysis and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.lines as mlines\n",
    "import pylab as plot\n",
    "import matplotlib\n",
    "import random\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import math\n",
    "import time\n",
    "\n",
    "### Import Scipy stats packages\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# import sklearn packages\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(style = 'whitegrid')\n",
    "sns.set_palette('bright')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all datasets to create master_df\n",
    "\n",
    "root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "df_aq = pd.read_csv(root + \"/data/cleaned/air_quality_NO2.csv\", index_col=0)[['value','latitude', 'longitude']]\n",
    "df_met = pd.read_csv(root + \"/data/cleaned/nO2_met.csv\", index_col=0)\n",
    "df_fac = pd.read_csv(root + \"/data/cleaned/no2_fac_data.csv\", index_col=0)\n",
    "# df_fac.drop(df_fac.columns[df_fac.columns.str.contains('_emsdist')], axis=1, inplace=True)\n",
    "df_traffic = pd.read_csv(root + \"/data/cleaned/intersection_final.csv\", index_col=0)\n",
    "\n",
    "df_m1 = df_aq.merge(df_met, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_m2 = df_m1.merge(df_fac, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_merged = df_m2.merge(df_traffic, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_merged.drop(columns = ['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "X = df_merged.drop(\"value\",1) \n",
    "y = df_merged[\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Modeling - Using Cross Validation and Randomized/Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB -- Code to get list of params for an estimator:\n",
    "#regressor.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scale and transform input data\n",
    "def get_data(X, y):\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    # scaling the data\n",
    "    feature_scaler = StandardScaler()\n",
    "    X = feature_scaler.fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "# acquiring transformed data\n",
    "X_arr, y_arr = get_data(X, y)\n",
    "cols = np.array(X.columns)\n",
    "\n",
    "# master list for all model scores\n",
    "model_scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_tuning(X_arr, y_arr, grid_param):\n",
    "    print(\"Tuning model\")\n",
    "    # defining the regressor\n",
    "    regressor = RandomForestRegressor(random_state=0)\n",
    "\n",
    "    # doing randomized search\n",
    "    rd_sr = RandomizedSearchCV(estimator=regressor,\n",
    "                               param_distributions=grid_param,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               cv=10,\n",
    "                               n_jobs=-1)\n",
    "\n",
    "    rd_sr.fit(X_arr, y_arr)\n",
    "\n",
    "    # best estimator and it's components\n",
    "    best_parameters = rd_sr.best_params_\n",
    "    best_est_score = rd_sr.best_score_\n",
    "    feature_imp = rd_sr.best_estimator_.feature_importances_\n",
    "    \n",
    "    return rd_sr, best_parameters, best_est_score, feature_imp\n",
    "    \n",
    "\n",
    "def rf_feat_selection(X_arr, y_arr, grid_param, cols):\n",
    "    rd_sr, best_parameters, best_est_score, feature_imp = rf_tuning(X_arr, y_arr, grid_param)\n",
    "    \n",
    "    print(\"Feature selection\")\n",
    "    # plotting feat imp histogram\n",
    "    pd.Series(feature_imp, index=cols).sort_values().plot(kind='barh')\n",
    "    \n",
    "    # calculating optimum number of features\n",
    "    high_score = float(-np.inf)\n",
    "    \n",
    "    for thresh in np.arange(min(feature_imp) + 0.002, max(feature_imp), 0.002):\n",
    "        # identify the most important features using threshold (use RFECV or top 10 percentile later)\n",
    "        sfm = SelectFromModel(rd_sr.best_estimator_, threshold=thresh, prefit=True)\n",
    "        selected_features = cols[sfm.get_support(indices=True)]\n",
    "        print(\"Number of selected features:\", len(selected_features))\n",
    "\n",
    "        # updated dataset\n",
    "        X_updated = sfm.transform(X_arr)\n",
    "\n",
    "        # train a second regressor on this new dataset with hyperparameters of best estimators above using CV\n",
    "        cv_score_list = cross_val_score(RandomForestRegressor(**best_parameters), X_updated, \n",
    "                                        y_arr, cv=10, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        if cv_score_list.mean() > high_score and len(selected_features) > 10:\n",
    "            high_score = cv_score_list.mean()\n",
    "            final_feat_lst = selected_features\n",
    "            feat_arr = X_updated\n",
    "            \n",
    "    return high_score, final_feat_lst, feat_arr, best_parameters, best_est_score\n",
    "\n",
    "\n",
    "def rf_model(X_arr, y_arr, grid_param, cols):\n",
    "    high_score, final_feat_lst, feat_arr, best_parameters, \\\n",
    "        best_est_score = rf_feat_selection(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "    print(\"Saving model to file\")\n",
    "    # saving the model to file\n",
    "    final_model = RandomForestRegressor(**best_parameters)\n",
    "    final_model.fit(feat_arr, y_arr)\n",
    "    pkl_filename = \"rf_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "\n",
    "    print(\"MSE before feature selection:\", best_est_score)\n",
    "    print(\"MSE after feature selection:\", high_score)\n",
    "    print(\"The final list of features are:\", final_feat_lst)\n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    \n",
    "    return high_score\n",
    "\n",
    "\n",
    "grid_param = {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [10, 20, 30, 40, 50],\n",
    "        #'min_samples_split': [2, 5, 10, 15, 20],\n",
    "        #'min_samples_leaf': [1, 2, 5, 10, 15],\n",
    "        #'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "\n",
    "# building the model\n",
    "final_score = rf_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Random Forest\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def xgb_tuning(X_arr, y_arr, grid_param):\n",
    "    print(\"Tuning XGB model\")\n",
    "    \n",
    "    # defining the regressor\n",
    "    \n",
    "    regressor = XGBRegressor(random_state=0)\n",
    "    \n",
    "    # RandomizedSearch\n",
    "    rd_sr = RandomizedSearchCV(estimator = regressor,\n",
    "                                   param_distributions = grid_param,\n",
    "                                   scoring = 'neg_mean_squared_error',\n",
    "                                   cv = 10,\n",
    "                                   n_jobs = -1\n",
    "                                  )\n",
    "    \n",
    "    rd_sr.fit(X_arr, y_arr)\n",
    "    \n",
    "    # best estimator and its components\n",
    "    best_parameters = rd_sr.best_params_\n",
    "    best_est_score = rd_sr.best_score_\n",
    "    feature_imp = rd_sr.best_estimator_.feature_importances_\n",
    "\n",
    "    return rd_sr, best_parameters, best_est_score, feature_imp\n",
    "\n",
    "def plot_hist(data):\n",
    "    plt.figure(figsize = (20,8))\n",
    "    ax = sns.barplot(y = 'Importance', x = 'Feature', \n",
    "                     hue = 'grouping',  \n",
    "                     data = data[:30],\n",
    "                     dodge = False, palette = 'muted')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.xlabel(\"Feature\", size = 20)\n",
    "    plt.xticks(size = 20)\n",
    "    plt.yticks(size = 16)\n",
    "    plt.ylabel(\"Importance\", size = 20)\n",
    "    plt.title(\"Feature selection using XGBoost and Cross Validation\", size = 20)\n",
    "    plt.show()\n",
    "\n",
    "def add_group(data):\n",
    "    data['grouping'] = \" \"\n",
    "    for index, string in enumerate(data['Feature']):\n",
    "        group = re.findall(r'-(.*?)-', string)\n",
    "        if group:\n",
    "            data.loc[index, 'grouping'] = group[0]\n",
    "        else:\n",
    "            data.loc[index, 'grouping'] = data.loc[index, 'Feature']\n",
    "    \n",
    "def xgb_feat_selection(X_arr, y_arr, grid_param, cols):\n",
    "    \n",
    "    rd_sr, best_parameters, best_est_score, feature_imp = xgb_tuning(\n",
    "        X_arr, y_arr, grid_param)\n",
    "    \n",
    "    print(\"Feature selection\")\n",
    "    # plotting feat imp histogram\n",
    "    sf = pd.Series(feature_imp, index=cols)\n",
    "    df = pd.DataFrame({'Feature':sf.index, 'Importance':sf.values})\n",
    "    df = df.sort_values('Importance', ascending = False).reset_index()\n",
    "    add_group(df)\n",
    "    plot_hist(df)\n",
    "    \n",
    "    # calculating optimum number of features\n",
    "    neg_score = float(-np.inf)\n",
    "    \n",
    "    for thresh in np.arange(min(feature_imp) + 0.002, max(feature_imp), 0.002):\n",
    "        # identify the most important features using threshold (use RFECV or top 10 percentile later)\n",
    "        sfm = SelectFromModel(rd_sr.best_estimator_, threshold=thresh, prefit=True)\n",
    "        selected_features = cols[sfm.get_support(indices=True)]\n",
    "        print(\"Number of selected features:\", len(selected_features))\n",
    "\n",
    "        # updated dataset\n",
    "        X_updated = sfm.transform(X_arr)\n",
    "\n",
    "        # train a second regressor on this new dataset with hyperparameters of best estimators above using CV\n",
    "        cv_score_list = cross_val_score(XGBRegressor(**best_parameters), X_updated, \n",
    "                                        y_arr, cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        if cv_score_list.mean() > neg_score and len(selected_features) > 10:\n",
    "            high_score = cv_score_list.mean()\n",
    "            final_feat_lst = selected_features\n",
    "            feat_arr = X_updated\n",
    "            \n",
    "    return high_score, final_feat_lst, feat_arr, best_parameters, best_est_score\n",
    "\n",
    "def xgb_model(X_arr, y_arr, grid_param, cols):\n",
    "    high_score, final_feat_lst, feat_arr, best_parameters, \\\n",
    "        best_est_score = xgb_feat_selection(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "    print(\"Saving model to file\")\n",
    "    # saving the model to file\n",
    "    final_model = XGBRegressor(**best_parameters)\n",
    "    final_model.fit(feat_arr, y_arr)\n",
    "    pkl_filename = \"xgb_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "\n",
    "    print(\"MSE before feature selection:\", best_est_score)\n",
    "    print(\"MSE after feature selection:\", high_score)\n",
    "    print(\"The final list of features are:\", final_feat_lst)\n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    \n",
    "    return high_score\n",
    "\n",
    "grid_param = {\n",
    "        'n_estimators': range(50, 500, 50),\n",
    "        'max_depth': range(5, 50, 5)\n",
    "    }\n",
    "\n",
    "\n",
    "# building the model\n",
    "final_score = xgb_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Extreme Gradient Boosting\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_model(X_arr, y_arr, grid_param, cols):\n",
    "    # defining the regressor\n",
    "    regressor = Lasso(random_state=1)\n",
    "\n",
    "    # do GridSearchCV\n",
    "    gd_sr = GridSearchCV(estimator=regressor,\n",
    "                        param_grid=grid_param,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=10,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "    gd_sr.fit(X_arr, y_arr)\n",
    "    best_parameters = gd_sr.best_params_\n",
    "    best_score = gd_sr.best_score_\n",
    "    \n",
    "    # feature importance list\n",
    "    coef = pd.Series(gd_sr.best_estimator_.coef_, index = cols)\n",
    "    print(\"Lasso picked \" + str(sum(coef != 0)) + \n",
    "      \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "    \n",
    "    # plotting the selected features\n",
    "    feat_imp = coef[coef != 0].sort_values()\n",
    "    plt.figure(figsize = (10, 15))\n",
    "    feat_imp.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature Importance using Lasso\")\n",
    "    plt.yticks(fontsize = 10)\n",
    "    plt.show()\n",
    "    \n",
    "    # saving the model to file\n",
    "    pkl_filename = \"lasso_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(gd_sr.best_estimator_, file)\n",
    "    \n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    print(\"The Neg. MSE after tuning is:\", best_score)\n",
    "    \n",
    "    return best_score\n",
    "\n",
    "\n",
    "grid_param = {\n",
    "            'alpha':[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "            }\n",
    "\n",
    "# building the model\n",
    "final_score = lasso_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Lasso\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_model(X_arr, y_arr, grid_param, cols):\n",
    "    # defining the regressor\n",
    "    regressor = Ridge(random_state=1)\n",
    "\n",
    "    # do GridSearchCV\n",
    "    gd_sr = GridSearchCV(estimator=regressor,\n",
    "                        param_grid=grid_param,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=10,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "    gd_sr.fit(X_arr, y_arr)\n",
    "    best_parameters = gd_sr.best_params_\n",
    "    best_score = gd_sr.best_score_\n",
    "    \n",
    "    # feature importance list\n",
    "    coef = pd.Series(gd_sr.best_estimator_.coef_, index = cols)\n",
    "    print(\"Ridge picked \" + str(sum(coef != 0)) + \n",
    "      \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "    \n",
    "    # plotting the selected features\n",
    "    feat_imp = coef[coef != 0].sort_values()\n",
    "    plt.figure(figsize = (10, 25))\n",
    "    feat_imp.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature Importance using Ridge\")\n",
    "    plt.yticks(fontsize = 10)\n",
    "    plt.show()\n",
    "    \n",
    "    # saving the model to file\n",
    "    pkl_filename = \"ridge_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(gd_sr.best_estimator_, file)\n",
    "    \n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    print(\"The Neg. MSE after tuning is:\", best_score)\n",
    "    \n",
    "    return best_score\n",
    "\n",
    "\n",
    "grid_param = {\n",
    "            'alpha':[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "            'solver':['auto', 'svd', 'cholesky', 'sag'] \n",
    "            }\n",
    "\n",
    "# building the model\n",
    "final_score = ridge_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Ridge\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Linear regression -- With and without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X_arr, y_arr, cols, pca_flag=False):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_arr, y_arr, \n",
    "                                                    test_size=0.3)\n",
    "    if pca_flag:\n",
    "        pca = PCA(.95)\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "        \n",
    "        # saving model to file\n",
    "        pkl_filename = \"pca.pkl\"\n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(pca, file)\n",
    "        \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = - mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # saving regression model to file\n",
    "    pkl_filename = \"linear_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    print(\"Neg. MSE is:\", score)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# building the linear model without PCA\n",
    "final_score = linear_model(X_arr, y_arr, cols)\n",
    "model_scores[\"Linear\"] = final_score\n",
    "\n",
    "# building the linear model with PCA feature selection\n",
    "final_score = linear_model(X_arr, y_arr, cols, pca_flag=True)\n",
    "model_scores[\"Linear(PCA)\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Neural regression via Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_tanh_stack): Sequential(\n",
      "    (0): Linear(in_features=82, out_features=60, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=60, out_features=40, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=40, out_features=30, bias=True)\n",
      "    (9): Tanh()\n",
      "    (10): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (11): Tanh()\n",
      "    (12): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (13): Tanh()\n",
      "    (14): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (15): Tanh()\n",
      "    (16): Linear(in_features=10, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Layer: linear_tanh_stack.0.weight | Size: torch.Size([60, 82]) | Values : tensor([[-0.0805,  0.0095,  0.0651, -0.0526, -0.0743, -0.0443,  0.0905, -0.0364,\n",
      "          0.0597, -0.0998,  0.0228,  0.0688,  0.0926, -0.0531, -0.0602,  0.0254,\n",
      "          0.0146,  0.0328,  0.0015, -0.0469, -0.0938,  0.0495,  0.0333, -0.0580,\n",
      "         -0.0489,  0.0238,  0.0304,  0.0472,  0.0575,  0.0006, -0.0555,  0.0620,\n",
      "         -0.0074,  0.0033, -0.0209,  0.0092,  0.0471, -0.0901, -0.1000, -0.0953,\n",
      "         -0.0983, -0.0384,  0.0096,  0.0594, -0.0023,  0.0452, -0.0047,  0.0455,\n",
      "         -0.0853, -0.0060, -0.0025,  0.0803, -0.1084,  0.0969,  0.0230,  0.0079,\n",
      "         -0.0933,  0.1093,  0.0246, -0.0450,  0.0519,  0.0294, -0.0742, -0.0449,\n",
      "          0.0762, -0.0144,  0.0309, -0.1050,  0.0888,  0.0991,  0.0347,  0.0167,\n",
      "         -0.0959,  0.1096,  0.0894,  0.0407,  0.0259,  0.0165, -0.0624, -0.0961,\n",
      "          0.0519, -0.0831],\n",
      "        [ 0.0173, -0.0061,  0.1094,  0.0903,  0.0348,  0.0774, -0.0703,  0.0908,\n",
      "         -0.0179, -0.1089, -0.0368, -0.0765, -0.0085, -0.0070,  0.0581,  0.0597,\n",
      "          0.0713, -0.0920,  0.0689, -0.0089, -0.0826,  0.1077, -0.0280, -0.1089,\n",
      "          0.0006,  0.0695, -0.1097, -0.0810, -0.0340, -0.0729,  0.0176,  0.0594,\n",
      "         -0.0632,  0.0549, -0.1085,  0.0298, -0.0096, -0.0869, -0.0571, -0.0226,\n",
      "         -0.0174,  0.0058,  0.0131, -0.0328,  0.0775,  0.0942, -0.0720,  0.0108,\n",
      "          0.0542, -0.0201,  0.1005, -0.0575,  0.0195, -0.0004, -0.0710,  0.0242,\n",
      "          0.1011, -0.0507, -0.0411, -0.1052, -0.0455, -0.0133,  0.0283,  0.0723,\n",
      "          0.0265, -0.0406, -0.0014, -0.0250,  0.0956,  0.0844,  0.0240, -0.0842,\n",
      "          0.0005,  0.0856,  0.0024,  0.0833,  0.0565, -0.0767,  0.0551, -0.0738,\n",
      "          0.0555,  0.0604]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.0.bias | Size: torch.Size([60]) | Values : tensor([-0.0670, -0.1048], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.2.weight | Size: torch.Size([60, 60]) | Values : tensor([[-0.0123,  0.0096, -0.0129, -0.0850, -0.1168, -0.1096,  0.0076, -0.1123,\n",
      "          0.1084,  0.0187,  0.0137,  0.0324, -0.0744, -0.0835, -0.1195, -0.0371,\n",
      "         -0.0418,  0.0742,  0.0925, -0.1104,  0.0107, -0.0003,  0.0875, -0.1273,\n",
      "         -0.0781, -0.0814, -0.0782,  0.0961,  0.0118, -0.0291, -0.1265,  0.0262,\n",
      "          0.0646,  0.0586,  0.0737,  0.1078, -0.0514,  0.0039, -0.0753, -0.1024,\n",
      "         -0.0232, -0.0261, -0.0478, -0.0203,  0.1151, -0.0971,  0.0611,  0.0716,\n",
      "         -0.0582, -0.0643,  0.1081, -0.1056,  0.0470,  0.0319, -0.1238,  0.0295,\n",
      "         -0.1215, -0.1077, -0.0874,  0.0155],\n",
      "        [ 0.1221, -0.1188,  0.0201,  0.0173,  0.0177, -0.0013,  0.0654,  0.0018,\n",
      "         -0.0459,  0.0123, -0.0826, -0.0713,  0.1213, -0.0830, -0.0297,  0.0150,\n",
      "         -0.0149,  0.1039, -0.0281,  0.0651, -0.0898, -0.0892, -0.1261,  0.0483,\n",
      "          0.0943,  0.0631, -0.0769,  0.0044, -0.1086,  0.0107,  0.0040, -0.0491,\n",
      "         -0.0571, -0.0290,  0.0429, -0.1095,  0.0162, -0.0784,  0.0956,  0.0746,\n",
      "          0.1223,  0.0181,  0.0833,  0.0844,  0.1199, -0.1040, -0.1006,  0.0612,\n",
      "         -0.0079,  0.0546, -0.0028,  0.0800,  0.1245, -0.1051, -0.0900, -0.0298,\n",
      "          0.0159, -0.0276, -0.0193,  0.0843]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.2.bias | Size: torch.Size([60]) | Values : tensor([0.0326, 0.0292], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.4.weight | Size: torch.Size([60, 60]) | Values : tensor([[ 0.0852, -0.0472, -0.0252, -0.0120, -0.0070,  0.0807, -0.0957,  0.0466,\n",
      "          0.0364, -0.0208, -0.1206,  0.0364,  0.0456,  0.0696, -0.1111, -0.0327,\n",
      "          0.1098, -0.0425,  0.0713, -0.0832,  0.1248, -0.0190,  0.0732,  0.0066,\n",
      "          0.0124, -0.1053,  0.0598,  0.0099, -0.0507, -0.1066, -0.0495, -0.0590,\n",
      "         -0.0773, -0.1177,  0.0880, -0.0280, -0.1138, -0.0462, -0.0288,  0.0732,\n",
      "         -0.1058,  0.1163, -0.0032,  0.1149,  0.0243, -0.1263,  0.0113, -0.0083,\n",
      "         -0.0682,  0.0529, -0.1212,  0.0397, -0.0221, -0.0535, -0.0258, -0.0622,\n",
      "         -0.0023, -0.0402,  0.0944, -0.0713],\n",
      "        [ 0.0872, -0.0357,  0.0911,  0.0185, -0.0459,  0.0757,  0.0085,  0.0178,\n",
      "          0.0983,  0.0294, -0.1164, -0.0423, -0.1176,  0.0240,  0.0969,  0.0725,\n",
      "         -0.0046,  0.1163,  0.0002,  0.0199, -0.1099,  0.0837, -0.0637,  0.0970,\n",
      "         -0.0097, -0.1232, -0.0290, -0.0466,  0.0478, -0.0972, -0.0882,  0.0876,\n",
      "         -0.0531,  0.0980,  0.1160,  0.0581,  0.0040,  0.0416,  0.0819,  0.0231,\n",
      "          0.0161, -0.0447, -0.0440,  0.0283,  0.1142, -0.1051,  0.0652,  0.0844,\n",
      "         -0.0145, -0.1285,  0.0107,  0.0006,  0.0977, -0.1183, -0.0686,  0.0130,\n",
      "          0.1221,  0.0786, -0.0647,  0.0090]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.4.bias | Size: torch.Size([60]) | Values : tensor([-0.0390, -0.0313], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.6.weight | Size: torch.Size([40, 60]) | Values : tensor([[-0.1002, -0.0817,  0.1196, -0.0751, -0.1107,  0.0987, -0.1203, -0.0796,\n",
      "         -0.1051,  0.0720, -0.0818, -0.0569, -0.0632, -0.0159,  0.1160, -0.0098,\n",
      "          0.0532, -0.1256, -0.0003, -0.0725,  0.0679, -0.0201,  0.0220,  0.0318,\n",
      "         -0.0878, -0.0221,  0.1089, -0.0906,  0.0954,  0.1154,  0.0164,  0.1056,\n",
      "         -0.0387, -0.1016,  0.0686,  0.0837,  0.0460,  0.0916,  0.0872, -0.1161,\n",
      "          0.0565, -0.0105, -0.1129, -0.0058, -0.0164,  0.0715, -0.0860,  0.0120,\n",
      "         -0.0747, -0.0731,  0.0367, -0.0249,  0.0488,  0.0222,  0.0821,  0.0138,\n",
      "         -0.0650,  0.1043,  0.0508,  0.0245],\n",
      "        [-0.0676,  0.0532,  0.1149, -0.0449, -0.0139, -0.0541,  0.0051, -0.0411,\n",
      "          0.0142, -0.0231,  0.0895,  0.0274,  0.0288, -0.0900, -0.0884,  0.0002,\n",
      "         -0.0084, -0.0669, -0.1129, -0.0492, -0.0599, -0.0983,  0.0495,  0.0023,\n",
      "          0.0593, -0.0687, -0.1011,  0.1224, -0.1030, -0.0617, -0.1118, -0.0734,\n",
      "          0.1087,  0.0215, -0.1254, -0.0932, -0.0265,  0.0216, -0.0834,  0.0194,\n",
      "         -0.0961,  0.0258, -0.0452,  0.0585,  0.0801,  0.0256, -0.0246, -0.0972,\n",
      "          0.0512,  0.0192, -0.1179,  0.1167, -0.0699, -0.0298,  0.0545,  0.0961,\n",
      "          0.1282,  0.0231, -0.0547,  0.0118]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.6.bias | Size: torch.Size([40]) | Values : tensor([-0.0087, -0.0222], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.8.weight | Size: torch.Size([30, 40]) | Values : tensor([[ 0.1131, -0.0807,  0.0201,  0.1433, -0.1046,  0.1311, -0.0338,  0.0103,\n",
      "         -0.1477,  0.0632,  0.1521,  0.0997, -0.1523,  0.0486, -0.0316, -0.0796,\n",
      "          0.0329,  0.0267, -0.0546,  0.0293, -0.0211, -0.0436, -0.0147,  0.1241,\n",
      "          0.1447,  0.0194, -0.0031,  0.1238,  0.0375,  0.0744,  0.0789,  0.0019,\n",
      "          0.0527,  0.1154,  0.0575, -0.0905, -0.0296, -0.1453, -0.0468, -0.1167],\n",
      "        [ 0.1529, -0.0721, -0.1018, -0.1007,  0.0446, -0.0575,  0.0049,  0.0259,\n",
      "          0.0989, -0.1272, -0.0272,  0.0074, -0.1545,  0.0286, -0.0370, -0.0153,\n",
      "         -0.1017, -0.0315,  0.0145,  0.1280, -0.1078, -0.0897, -0.1266,  0.0790,\n",
      "         -0.0385,  0.1185, -0.1004,  0.0580,  0.1448,  0.1459, -0.1434,  0.0788,\n",
      "          0.1128, -0.1149, -0.0816,  0.0747,  0.0566,  0.0528, -0.0070,  0.0560]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.8.bias | Size: torch.Size([30]) | Values : tensor([0.0771, 0.1505], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.10.weight | Size: torch.Size([20, 30]) | Values : tensor([[ 0.0487,  0.1648, -0.1207,  0.1201,  0.1665,  0.0600,  0.0239,  0.0509,\n",
      "          0.1310, -0.0944, -0.0324,  0.0209, -0.1125,  0.0829, -0.1640,  0.0097,\n",
      "          0.0496, -0.1777,  0.0774,  0.0270,  0.1503,  0.1083,  0.0221, -0.1599,\n",
      "          0.0873, -0.1376,  0.1760, -0.1535,  0.0241, -0.0521],\n",
      "        [-0.1222, -0.0374, -0.1655, -0.0816,  0.0079, -0.1001, -0.1520, -0.1770,\n",
      "         -0.1724, -0.0425,  0.0447, -0.1238,  0.1056, -0.1068,  0.1471,  0.1257,\n",
      "         -0.1682, -0.1396,  0.0960,  0.0122,  0.1144,  0.1374,  0.0810,  0.0528,\n",
      "          0.0162,  0.0613,  0.0852, -0.0926,  0.1749, -0.1496]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.10.bias | Size: torch.Size([20]) | Values : tensor([-0.1365, -0.0474], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.12.weight | Size: torch.Size([20, 20]) | Values : tensor([[-0.0460,  0.0636, -0.1165, -0.0729,  0.1474, -0.0160,  0.0888,  0.1850,\n",
      "          0.0495, -0.0543, -0.2159, -0.0855, -0.2000,  0.1664, -0.1646, -0.1561,\n",
      "          0.0945,  0.1303, -0.0323, -0.1640],\n",
      "        [-0.1625,  0.2223,  0.0168, -0.2056,  0.1181,  0.0384, -0.0618, -0.1184,\n",
      "         -0.0359, -0.1568,  0.1039,  0.0223,  0.1461,  0.0700, -0.2079,  0.0926,\n",
      "          0.0877, -0.0206, -0.2154, -0.0643]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.12.bias | Size: torch.Size([20]) | Values : tensor([-0.1572, -0.1587], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.14.weight | Size: torch.Size([10, 20]) | Values : tensor([[ 0.0812,  0.2128, -0.1434, -0.1827, -0.0748,  0.1720,  0.1528,  0.0894,\n",
      "         -0.1561, -0.1020, -0.1676, -0.0831,  0.1530,  0.1919, -0.1969, -0.1303,\n",
      "         -0.1414,  0.0063,  0.0794, -0.1898],\n",
      "        [-0.1227, -0.1523,  0.0216,  0.1915,  0.2124, -0.0649,  0.1390, -0.0018,\n",
      "         -0.1582,  0.0705,  0.0702,  0.0287, -0.0831, -0.1635,  0.0054, -0.2214,\n",
      "         -0.0282,  0.1840, -0.0164,  0.1155]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.14.bias | Size: torch.Size([10]) | Values : tensor([-0.1068, -0.0675], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.16.weight | Size: torch.Size([1, 10]) | Values : tensor([[-0.1675,  0.0504, -0.2963,  0.2634,  0.2726,  0.0677, -0.1876, -0.0169,\n",
      "         -0.0886,  0.1693]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.16.bias | Size: torch.Size([1]) | Values : tensor([0.1456], grad_fn=<SliceBackward>) \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.005476716905832291  [0/7511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.731450306077022e-06  [500/7511]\n",
      "loss: 4.6754669824622397e-07  [1000/7511]\n",
      "loss: 7.787539857417869e-07  [1500/7511]\n",
      "loss: 8.923670407057216e-07  [2000/7511]\n",
      "loss: 1.2033410712319892e-06  [2500/7511]\n",
      "loss: 1.9136161881760927e-06  [3000/7511]\n",
      "loss: 1.3867007453427505e-07  [3500/7511]\n",
      "loss: 1.2663155075642862e-06  [4000/7511]\n",
      "loss: 3.462430413492257e-07  [4500/7511]\n",
      "loss: 3.450624319611961e-07  [5000/7511]\n",
      "loss: 9.605700768133829e-08  [5500/7511]\n",
      "loss: 2.480922375980299e-06  [6000/7511]\n",
      "loss: 1.2046288588862808e-07  [6500/7511]\n",
      "loss: 1.1306823921586329e-07  [7000/7511]\n",
      "loss: 1.9767850290008937e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.959531109194407e-07 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 8.47217052069027e-08  [0/7511]\n",
      "loss: 1.2495624446273723e-07  [500/7511]\n",
      "loss: 1.3523616360089363e-07  [1000/7511]\n",
      "loss: 6.301005441855523e-07  [1500/7511]\n",
      "loss: 3.016554614987399e-07  [2000/7511]\n",
      "loss: 1.9676215856634371e-07  [2500/7511]\n",
      "loss: 3.7218976558506256e-06  [3000/7511]\n",
      "loss: 5.626203574138344e-07  [3500/7511]\n",
      "loss: 1.2446699315660226e-07  [4000/7511]\n",
      "loss: 2.596696049295133e-07  [4500/7511]\n",
      "loss: 2.8726399250444956e-06  [5000/7511]\n",
      "loss: 4.4171818558425e-07  [5500/7511]\n",
      "loss: 2.2788942999341089e-07  [6000/7511]\n",
      "loss: 8.898947356783538e-08  [6500/7511]\n",
      "loss: 4.641390205506468e-07  [7000/7511]\n",
      "loss: 1.0670629535525222e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.712410779791157e-07 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 8.75341470418789e-08  [0/7511]\n",
      "loss: 8.869047007920017e-08  [500/7511]\n",
      "loss: 3.780909878514649e-07  [1000/7511]\n",
      "loss: 1.7155807086055574e-07  [1500/7511]\n",
      "loss: 1.4005767923208623e-07  [2000/7511]\n",
      "loss: 1.807824645538858e-07  [2500/7511]\n",
      "loss: 1.0969615971134772e-07  [3000/7511]\n",
      "loss: 6.191982038217247e-07  [3500/7511]\n",
      "loss: 2.0986597348837677e-08  [4000/7511]\n",
      "loss: 1.5053852848723182e-06  [4500/7511]\n",
      "loss: 1.5121803187412297e-07  [5000/7511]\n",
      "loss: 8.255000807366741e-07  [5500/7511]\n",
      "loss: 8.757968572581376e-08  [6000/7511]\n",
      "loss: 6.030198846929125e-07  [6500/7511]\n",
      "loss: 1.4922532898253849e-07  [7000/7511]\n",
      "loss: 1.4544440318786656e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.586776122689724e-07 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 5.448049122946941e-08  [0/7511]\n",
      "loss: 1.3814482713314646e-07  [500/7511]\n",
      "loss: 2.0896142416404473e-07  [1000/7511]\n",
      "loss: 2.1166752617318707e-07  [1500/7511]\n",
      "loss: 1.1302252289624448e-07  [2000/7511]\n",
      "loss: 6.646152428402274e-07  [2500/7511]\n",
      "loss: 8.56622577316557e-08  [3000/7511]\n",
      "loss: 2.008663813057865e-07  [3500/7511]\n",
      "loss: 2.0894378849334316e-07  [4000/7511]\n",
      "loss: 7.123095713268413e-08  [4500/7511]\n",
      "loss: 6.203329121490242e-07  [5000/7511]\n",
      "loss: 8.360554915043394e-08  [5500/7511]\n",
      "loss: 7.341594709942001e-07  [6000/7511]\n",
      "loss: 5.6983395779752755e-08  [6500/7511]\n",
      "loss: 1.5422124306496698e-06  [7000/7511]\n",
      "loss: 2.4567034984102065e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.482092479265854e-07 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.009856871154625e-06  [0/7511]\n",
      "loss: 3.00395481644955e-06  [500/7511]\n",
      "loss: 9.458429417463776e-08  [1000/7511]\n",
      "loss: 1.6004521796730842e-07  [1500/7511]\n",
      "loss: 1.277399519494793e-07  [2000/7511]\n",
      "loss: 5.7783886120432726e-08  [2500/7511]\n",
      "loss: 7.970901947373932e-07  [3000/7511]\n",
      "loss: 5.567674747908313e-07  [3500/7511]\n",
      "loss: 2.3690741102200263e-07  [4000/7511]\n",
      "loss: 5.475846478475432e-07  [4500/7511]\n",
      "loss: 2.2095581186931668e-07  [5000/7511]\n",
      "loss: 2.1317269727205712e-07  [5500/7511]\n",
      "loss: 1.0546504114472555e-07  [6000/7511]\n",
      "loss: 1.732409486976394e-07  [6500/7511]\n",
      "loss: 5.472867314892937e-07  [7000/7511]\n",
      "loss: 1.8540978885539516e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.426192204132414e-07 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 4.4904763853992335e-06  [0/7511]\n",
      "loss: 3.0496803447022103e-06  [500/7511]\n",
      "loss: 7.842143645575561e-08  [1000/7511]\n",
      "loss: 1.5333783949245117e-06  [1500/7511]\n",
      "loss: 8.655521099854013e-08  [2000/7511]\n",
      "loss: 1.2058430343131477e-07  [2500/7511]\n",
      "loss: 6.078823844291037e-08  [3000/7511]\n",
      "loss: 4.9348855668540637e-08  [3500/7511]\n",
      "loss: 1.2199558341308148e-07  [4000/7511]\n",
      "loss: 8.40231777488043e-08  [4500/7511]\n",
      "loss: 4.6038422851779615e-07  [5000/7511]\n",
      "loss: 2.9891936037529376e-07  [5500/7511]\n",
      "loss: 5.999254995003866e-07  [6000/7511]\n",
      "loss: 6.044775204827602e-07  [6500/7511]\n",
      "loss: 1.2634230017738446e-07  [7000/7511]\n",
      "loss: 1.3674649608219624e-06  [7500/7511]\n",
      "Test Error: Avg loss: 5.40404659972072e-07 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.0012360718292257e-07  [0/7511]\n",
      "loss: 3.723662587162835e-07  [500/7511]\n",
      "loss: 1.5041966889839387e-06  [1000/7511]\n",
      "loss: 9.173582782295853e-08  [1500/7511]\n",
      "loss: 1.631488402153991e-07  [2000/7511]\n",
      "loss: 2.4374662643822376e-07  [2500/7511]\n",
      "loss: 1.5937744137772825e-06  [3000/7511]\n",
      "loss: 1.1958337609030423e-07  [3500/7511]\n",
      "loss: 3.402552692932659e-07  [4000/7511]\n",
      "loss: 7.301492814804078e-07  [4500/7511]\n",
      "loss: 5.386000339058228e-07  [5000/7511]\n",
      "loss: 2.1757804802291503e-07  [5500/7511]\n",
      "loss: 8.499991963617504e-07  [6000/7511]\n",
      "loss: 1.7002436436541757e-07  [6500/7511]\n",
      "loss: 2.654726642958849e-07  [7000/7511]\n",
      "loss: 1.5225252525397082e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.333991014667893e-07 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 4.157961939199595e-07  [0/7511]\n",
      "loss: 1.9390805050534254e-07  [500/7511]\n",
      "loss: 7.629506626471994e-07  [1000/7511]\n",
      "loss: 1.0614979828460491e-07  [1500/7511]\n",
      "loss: 1.6235863142810558e-07  [2000/7511]\n",
      "loss: 1.8773397414406645e-07  [2500/7511]\n",
      "loss: 5.767293487224379e-07  [3000/7511]\n",
      "loss: 9.742532824930095e-08  [3500/7511]\n",
      "loss: 9.033792025547882e-08  [4000/7511]\n",
      "loss: 1.5926227661111625e-06  [4500/7511]\n",
      "loss: 4.4494868234323803e-07  [5000/7511]\n",
      "loss: 1.2759315382027125e-07  [5500/7511]\n",
      "loss: 5.3225516438715204e-08  [6000/7511]\n",
      "loss: 2.494417401521787e-07  [6500/7511]\n",
      "loss: 5.868796506547369e-07  [7000/7511]\n",
      "loss: 1.2240079172443075e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.253724061838876e-07 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 4.585132842294115e-07  [0/7511]\n",
      "loss: 2.717282541198074e-06  [500/7511]\n",
      "loss: 1.9212784252431447e-07  [1000/7511]\n",
      "loss: 8.592235190008068e-07  [1500/7511]\n",
      "loss: 1.3098144791001687e-07  [2000/7511]\n",
      "loss: 1.6370820503652794e-06  [2500/7511]\n",
      "loss: 3.615131447531894e-07  [3000/7511]\n",
      "loss: 2.4958316657830437e-07  [3500/7511]\n",
      "loss: 1.4569808115538763e-07  [4000/7511]\n",
      "loss: 7.167254523210431e-08  [4500/7511]\n",
      "loss: 1.6392699819789414e-07  [5000/7511]\n",
      "loss: 5.8442068961994664e-08  [5500/7511]\n",
      "loss: 2.0136225487021875e-07  [6000/7511]\n",
      "loss: 1.0570351349770135e-07  [6500/7511]\n",
      "loss: 6.033933885873921e-08  [7000/7511]\n",
      "loss: 7.971016202645842e-08  [7500/7511]\n",
      "Test Error: Avg loss: 5.218048289523244e-07 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.4005173909481528e-07  [0/7511]\n",
      "loss: 2.330113602511119e-06  [500/7511]\n",
      "loss: 2.149831288988935e-06  [1000/7511]\n",
      "loss: 1.7304366792814108e-06  [1500/7511]\n",
      "loss: 8.761117555877718e-07  [2000/7511]\n",
      "loss: 2.3745231203520234e-07  [2500/7511]\n",
      "loss: 1.7470425461851846e-07  [3000/7511]\n",
      "loss: 1.1181560921613709e-07  [3500/7511]\n",
      "loss: 4.08682308261632e-06  [4000/7511]\n",
      "loss: 9.792798039143236e-08  [4500/7511]\n",
      "loss: 3.7423612297970976e-07  [5000/7511]\n",
      "loss: 2.736410351644736e-07  [5500/7511]\n",
      "loss: 4.187440083569527e-07  [6000/7511]\n",
      "loss: 2.44122446702022e-07  [6500/7511]\n",
      "loss: 2.41967001102239e-07  [7000/7511]\n",
      "loss: 7.171926199589507e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.118725876661698e-07 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.6515765821623063e-07  [0/7511]\n",
      "loss: 8.313061243825359e-07  [500/7511]\n",
      "loss: 2.7167811822437216e-07  [1000/7511]\n",
      "loss: 4.2753140405693557e-07  [1500/7511]\n",
      "loss: 1.2783563363427675e-07  [2000/7511]\n",
      "loss: 9.947461876436137e-08  [2500/7511]\n",
      "loss: 3.0435211328949663e-07  [3000/7511]\n",
      "loss: 4.96280243567071e-08  [3500/7511]\n",
      "loss: 4.6392247554649657e-07  [4000/7511]\n",
      "loss: 4.311326620154432e-07  [4500/7511]\n",
      "loss: 2.3736215553071816e-06  [5000/7511]\n",
      "loss: 2.3918161673464056e-07  [5500/7511]\n",
      "loss: 1.665657265448317e-07  [6000/7511]\n",
      "loss: 5.190564422719035e-08  [6500/7511]\n",
      "loss: 1.208978162026142e-08  [7000/7511]\n",
      "loss: 1.6618467668649828e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.11930912725238e-07 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 9.378471332865956e-08  [0/7511]\n",
      "loss: 6.538994057336822e-07  [500/7511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.8954391123315872e-07  [1000/7511]\n",
      "loss: 1.2776336006936617e-06  [1500/7511]\n",
      "loss: 1.388392263379501e-07  [2000/7511]\n",
      "loss: 5.982128925552388e-08  [2500/7511]\n",
      "loss: 1.009273731256144e-07  [3000/7511]\n",
      "loss: 3.165710893426876e-08  [3500/7511]\n",
      "loss: 6.214977332774652e-08  [4000/7511]\n",
      "loss: 8.169969873961236e-07  [4500/7511]\n",
      "loss: 6.473675995266603e-08  [5000/7511]\n",
      "loss: 1.5560551673843293e-06  [5500/7511]\n",
      "loss: 1.4968989603403315e-07  [6000/7511]\n",
      "loss: 1.3801623026665766e-06  [6500/7511]\n",
      "loss: 1.8439075688547746e-07  [7000/7511]\n",
      "loss: 1.1607511396505288e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.158031383764738e-07 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.0763889122245018e-06  [0/7511]\n",
      "loss: 2.0113724019665824e-07  [500/7511]\n",
      "loss: 9.84238894830014e-08  [1000/7511]\n",
      "loss: 9.856619698211944e-08  [1500/7511]\n",
      "loss: 1.0139970640921092e-07  [2000/7511]\n",
      "loss: 7.845439853326752e-08  [2500/7511]\n",
      "loss: 2.0666203681685147e-07  [3000/7511]\n",
      "loss: 4.051090911616484e-07  [3500/7511]\n",
      "loss: 9.55010932557343e-08  [4000/7511]\n",
      "loss: 2.715898119731719e-07  [4500/7511]\n",
      "loss: 2.4373849782932666e-07  [5000/7511]\n",
      "loss: 5.478333946484781e-07  [5500/7511]\n",
      "loss: 4.0851799099073105e-07  [6000/7511]\n",
      "loss: 2.079193109238986e-06  [6500/7511]\n",
      "loss: 5.4679492222931e-07  [7000/7511]\n",
      "loss: 1.8539304846854066e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.152322174565479e-07 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.7527330530574545e-06  [0/7511]\n",
      "loss: 4.257735852775113e-08  [500/7511]\n",
      "loss: 3.3141599686814516e-08  [1000/7511]\n",
      "loss: 2.7772575776907615e-06  [1500/7511]\n",
      "loss: 2.2990556658442074e-07  [2000/7511]\n",
      "loss: 9.076886158254638e-07  [2500/7511]\n",
      "loss: 2.0526334765236243e-07  [3000/7511]\n",
      "loss: 2.643171512772824e-07  [3500/7511]\n",
      "loss: 2.3836028049117886e-06  [4000/7511]\n",
      "loss: 3.9053270484146196e-07  [4500/7511]\n",
      "loss: 3.4133802273572655e-07  [5000/7511]\n",
      "loss: 1.6344691289305047e-07  [5500/7511]\n",
      "loss: 4.070340864359423e-08  [6000/7511]\n",
      "loss: 6.39285033798842e-08  [6500/7511]\n",
      "loss: 9.045378845939922e-08  [7000/7511]\n",
      "loss: 9.773813758329197e-08  [7500/7511]\n",
      "Test Error: Avg loss: 5.13977703928292e-07 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 3.6386009583111445e-07  [0/7511]\n",
      "loss: 6.955569347155688e-07  [500/7511]\n",
      "loss: 7.951891944912859e-08  [1000/7511]\n",
      "loss: 1.0707991577874054e-06  [1500/7511]\n",
      "loss: 3.6250744983590266e-07  [2000/7511]\n",
      "loss: 7.707002680490405e-08  [2500/7511]\n",
      "loss: 8.511206601724552e-07  [3000/7511]\n",
      "loss: 2.2220410755835474e-07  [3500/7511]\n",
      "loss: 2.867791408789344e-06  [4000/7511]\n",
      "loss: 1.0701796071543868e-07  [4500/7511]\n",
      "loss: 4.81764686810493e-07  [5000/7511]\n",
      "loss: 1.9583985704230145e-06  [5500/7511]\n",
      "loss: 2.2596718451950437e-07  [6000/7511]\n",
      "loss: 1.2046015740452276e-07  [6500/7511]\n",
      "loss: 2.388431710187433e-07  [7000/7511]\n",
      "loss: 1.757322820594709e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.055449599317589e-07 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 3.238191368382104e-07  [0/7511]\n",
      "loss: 3.825360863629612e-07  [500/7511]\n",
      "loss: 1.351649956404799e-07  [1000/7511]\n",
      "loss: 5.860218266207085e-07  [1500/7511]\n",
      "loss: 1.3794947051337658e-07  [2000/7511]\n",
      "loss: 2.3429467432833917e-07  [2500/7511]\n",
      "loss: 2.2134075550184207e-07  [3000/7511]\n",
      "loss: 1.7621764527575579e-06  [3500/7511]\n",
      "loss: 5.427185101325449e-08  [4000/7511]\n",
      "loss: 1.9093769765277102e-07  [4500/7511]\n",
      "loss: 3.1744119155519e-07  [5000/7511]\n",
      "loss: 6.807430281696725e-07  [5500/7511]\n",
      "loss: 1.501092015132599e-07  [6000/7511]\n",
      "loss: 1.7402199148364161e-07  [6500/7511]\n",
      "loss: 2.85288109580506e-07  [7000/7511]\n",
      "loss: 1.5982288914528908e-07  [7500/7511]\n",
      "Test Error: Avg loss: 5.061136908354006e-07 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.607198439363856e-06  [0/7511]\n",
      "loss: 1.2126811554935557e-07  [500/7511]\n",
      "loss: 1.0625957713727985e-07  [1000/7511]\n",
      "loss: 1.4180898233462358e-07  [1500/7511]\n",
      "loss: 2.7065777885582065e-07  [2000/7511]\n",
      "loss: 1.7551343489685678e-07  [2500/7511]\n",
      "loss: 1.1242762099072934e-07  [3000/7511]\n",
      "loss: 8.355301019946637e-07  [3500/7511]\n",
      "loss: 4.4176496771797247e-07  [4000/7511]\n",
      "loss: 4.344654371379875e-06  [4500/7511]\n",
      "loss: 4.062299296947458e-07  [5000/7511]\n",
      "loss: 7.307798455258308e-07  [5500/7511]\n",
      "loss: 1.6605387145318673e-06  [6000/7511]\n",
      "loss: 4.263796427039779e-07  [6500/7511]\n",
      "loss: 9.77769119003824e-08  [7000/7511]\n",
      "loss: 5.8489366239200535e-08  [7500/7511]\n",
      "Test Error: Avg loss: 4.94517058444666e-07 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.1094903956964117e-07  [0/7511]\n",
      "loss: 1.2442496881703846e-06  [500/7511]\n",
      "loss: 1.1948067140110652e-06  [1000/7511]\n",
      "loss: 1.5562429211968265e-07  [1500/7511]\n",
      "loss: 2.739104445481644e-07  [2000/7511]\n",
      "loss: 1.138203913342295e-07  [2500/7511]\n",
      "loss: 1.7136106862380984e-06  [3000/7511]\n",
      "loss: 2.6622558380040573e-07  [3500/7511]\n",
      "loss: 2.3083964606485097e-07  [4000/7511]\n",
      "loss: 2.361357047675483e-07  [4500/7511]\n",
      "loss: 9.564898562075541e-08  [5000/7511]\n",
      "loss: 2.67705246415062e-07  [5500/7511]\n",
      "loss: 2.891794395054603e-07  [6000/7511]\n",
      "loss: 6.158080623208662e-08  [6500/7511]\n",
      "loss: 4.12600797972118e-07  [7000/7511]\n",
      "loss: 1.8137644985927182e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.954654201699102e-07 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 5.641452958116133e-07  [0/7511]\n",
      "loss: 1.762913939273858e-07  [500/7511]\n",
      "loss: 4.2290980672987644e-07  [1000/7511]\n",
      "loss: 9.587109843778308e-07  [1500/7511]\n",
      "loss: 2.137139176738856e-07  [2000/7511]\n",
      "loss: 2.461246594975819e-06  [2500/7511]\n",
      "loss: 8.804386197880376e-08  [3000/7511]\n",
      "loss: 5.91499578206367e-08  [3500/7511]\n",
      "loss: 1.2796040493867622e-07  [4000/7511]\n",
      "loss: 1.6377016720525717e-07  [4500/7511]\n",
      "loss: 1.586372064821262e-07  [5000/7511]\n",
      "loss: 2.127268089680001e-06  [5500/7511]\n",
      "loss: 2.55314671449014e-06  [6000/7511]\n",
      "loss: 1.2790802372819599e-07  [6500/7511]\n",
      "loss: 1.2509579505604052e-07  [7000/7511]\n",
      "loss: 6.431715746657574e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.962693817687228e-07 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.432362180115888e-06  [0/7511]\n",
      "loss: 8.090709258112838e-08  [500/7511]\n",
      "loss: 9.33257595647774e-08  [1000/7511]\n",
      "loss: 2.381255228556256e-07  [1500/7511]\n",
      "loss: 1.568671592622195e-07  [2000/7511]\n",
      "loss: 3.9819272501517844e-07  [2500/7511]\n",
      "loss: 3.5699315503734397e-07  [3000/7511]\n",
      "loss: 1.4885158350352867e-07  [3500/7511]\n",
      "loss: 3.119125722150784e-06  [4000/7511]\n",
      "loss: 2.5650647330621723e-06  [4500/7511]\n",
      "loss: 4.410682308275682e-08  [5000/7511]\n",
      "loss: 3.584463570405205e-08  [5500/7511]\n",
      "loss: 1.1514058542161365e-06  [6000/7511]\n",
      "loss: 2.1957515627946123e-07  [6500/7511]\n",
      "loss: 9.209534113097106e-08  [7000/7511]\n",
      "loss: 8.041641308409453e-08  [7500/7511]\n",
      "Test Error: Avg loss: 4.94248166388357e-07 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 9.370806424158218e-07  [0/7511]\n",
      "loss: 9.383409604879489e-08  [500/7511]\n",
      "loss: 1.3669344411937345e-07  [1000/7511]\n",
      "loss: 1.2332384358160198e-07  [1500/7511]\n",
      "loss: 6.842272455287457e-07  [2000/7511]\n",
      "loss: 2.733679878019757e-07  [2500/7511]\n",
      "loss: 2.3182285247003165e-07  [3000/7511]\n",
      "loss: 1.5265341346548666e-07  [3500/7511]\n",
      "loss: 1.13004284685303e-06  [4000/7511]\n",
      "loss: 8.933884743100862e-08  [4500/7511]\n",
      "loss: 7.876688812302746e-08  [5000/7511]\n",
      "loss: 1.5559466248760145e-07  [5500/7511]\n",
      "loss: 1.2244237268532743e-07  [6000/7511]\n",
      "loss: 8.016431252144685e-08  [6500/7511]\n",
      "loss: 1.3631170077132992e-06  [7000/7511]\n",
      "loss: 6.19755553543655e-08  [7500/7511]\n",
      "Test Error: Avg loss: 4.89442800659146e-07 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.1443924563536712e-07  [0/7511]\n",
      "loss: 1.0503567864361685e-06  [500/7511]\n",
      "loss: 3.325007469356933e-07  [1000/7511]\n",
      "loss: 5.847806505698827e-07  [1500/7511]\n",
      "loss: 2.6820961807061394e-07  [2000/7511]\n",
      "loss: 1.2249046221768367e-07  [2500/7511]\n",
      "loss: 1.6211261026910506e-06  [3000/7511]\n",
      "loss: 5.295695473250817e-07  [3500/7511]\n",
      "loss: 2.4097877826534386e-07  [4000/7511]\n",
      "loss: 1.5492796592297964e-06  [4500/7511]\n",
      "loss: 8.740578323340742e-07  [5000/7511]\n",
      "loss: 5.160250680091849e-07  [5500/7511]\n",
      "loss: 7.702638527007366e-07  [6000/7511]\n",
      "loss: 3.070935008508968e-07  [6500/7511]\n",
      "loss: 7.963124062371207e-07  [7000/7511]\n",
      "loss: 3.541748583302251e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.912829899389149e-07 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 6.765772297967487e-08  [0/7511]\n",
      "loss: 1.5499130086027435e-06  [500/7511]\n",
      "loss: 3.504895005335129e-07  [1000/7511]\n",
      "loss: 6.239040573063903e-08  [1500/7511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.53948911677071e-07  [2000/7511]\n",
      "loss: 5.650754815178516e-07  [2500/7511]\n",
      "loss: 1.2994755138606706e-07  [3000/7511]\n",
      "loss: 2.766136333320901e-07  [3500/7511]\n",
      "loss: 1.67002369266811e-07  [4000/7511]\n",
      "loss: 9.419283486522545e-08  [4500/7511]\n",
      "loss: 1.398718012524114e-07  [5000/7511]\n",
      "loss: 3.126240244455403e-08  [5500/7511]\n",
      "loss: 1.8070723228902352e-07  [6000/7511]\n",
      "loss: 1.5533298380887572e-07  [6500/7511]\n",
      "loss: 2.96284383694001e-06  [7000/7511]\n",
      "loss: 1.2196996124202997e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.88359742234283e-07 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 3.294313799528936e-08  [0/7511]\n",
      "loss: 2.55628577860989e-07  [500/7511]\n",
      "loss: 2.1000447247843113e-07  [1000/7511]\n",
      "loss: 1.4674594694952248e-06  [1500/7511]\n",
      "loss: 8.50660413220794e-08  [2000/7511]\n",
      "loss: 2.691648887775955e-06  [2500/7511]\n",
      "loss: 6.088880866172985e-08  [3000/7511]\n",
      "loss: 8.007648233387954e-08  [3500/7511]\n",
      "loss: 8.075748496594315e-07  [4000/7511]\n",
      "loss: 1.121194031838968e-06  [4500/7511]\n",
      "loss: 1.8688295995161752e-06  [5000/7511]\n",
      "loss: 2.486062555817625e-07  [5500/7511]\n",
      "loss: 7.706058227086032e-07  [6000/7511]\n",
      "loss: 5.9047401634870766e-08  [6500/7511]\n",
      "loss: 2.921777422670857e-06  [7000/7511]\n",
      "loss: 2.5791607072278566e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.85013094283111e-07 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 1.609489117981866e-06  [0/7511]\n",
      "loss: 1.5140111031541892e-07  [500/7511]\n",
      "loss: 7.94417019278626e-07  [1000/7511]\n",
      "loss: 5.635989097640959e-08  [1500/7511]\n",
      "loss: 6.186851209122324e-08  [2000/7511]\n",
      "loss: 2.4801695985843253e-07  [2500/7511]\n",
      "loss: 7.202250458249182e-07  [3000/7511]\n",
      "loss: 3.907170054162634e-08  [3500/7511]\n",
      "loss: 2.778557757210365e-07  [4000/7511]\n",
      "loss: 3.3421417811041465e-07  [4500/7511]\n",
      "loss: 6.204717806213011e-08  [5000/7511]\n",
      "loss: 7.265744983442346e-08  [5500/7511]\n",
      "loss: 3.342948105000687e-07  [6000/7511]\n",
      "loss: 3.7751726722490275e-07  [6500/7511]\n",
      "loss: 3.094669693837204e-07  [7000/7511]\n",
      "loss: 3.2593069931863283e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.859009918715522e-07 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.486397931988904e-07  [0/7511]\n",
      "loss: 4.201745795739953e-08  [500/7511]\n",
      "loss: 2.037053491221741e-06  [1000/7511]\n",
      "loss: 1.8544895397099026e-07  [1500/7511]\n",
      "loss: 1.8197327733560087e-07  [2000/7511]\n",
      "loss: 1.5399734820675803e-07  [2500/7511]\n",
      "loss: 1.569694404679467e-06  [3000/7511]\n",
      "loss: 1.20851794349619e-07  [3500/7511]\n",
      "loss: 9.56430085352622e-07  [4000/7511]\n",
      "loss: 7.752088322376949e-07  [4500/7511]\n",
      "loss: 1.6330893970462057e-07  [5000/7511]\n",
      "loss: 1.2675978666720766e-07  [5500/7511]\n",
      "loss: 4.051259452353406e-07  [6000/7511]\n",
      "loss: 5.434863226128073e-08  [6500/7511]\n",
      "loss: 2.027759819611674e-06  [7000/7511]\n",
      "loss: 1.2447138431070925e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.872640894070184e-07 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 6.686170763714472e-07  [0/7511]\n",
      "loss: 1.8540238500008854e-07  [500/7511]\n",
      "loss: 1.7827085230237572e-06  [1000/7511]\n",
      "loss: 7.959286563163914e-08  [1500/7511]\n",
      "loss: 5.082625875729718e-07  [2000/7511]\n",
      "loss: 1.0710782305523026e-07  [2500/7511]\n",
      "loss: 1.9139736195938895e-06  [3000/7511]\n",
      "loss: 2.309505475750484e-07  [3500/7511]\n",
      "loss: 1.1877921934910773e-07  [4000/7511]\n",
      "loss: 6.395746368070832e-07  [4500/7511]\n",
      "loss: 3.306872429220675e-07  [5000/7511]\n",
      "loss: 2.653930941676208e-08  [5500/7511]\n",
      "loss: 2.5645902042015223e-06  [6000/7511]\n",
      "loss: 1.655133473832393e-06  [6500/7511]\n",
      "loss: 1.2462569998206163e-07  [7000/7511]\n",
      "loss: 1.2346765743131982e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.84048892139173e-07 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 7.01885838338967e-08  [0/7511]\n",
      "loss: 4.6697496713932196e-07  [500/7511]\n",
      "loss: 1.0756531310107675e-07  [1000/7511]\n",
      "loss: 3.3362292128913396e-07  [1500/7511]\n",
      "loss: 1.2767412727043848e-07  [2000/7511]\n",
      "loss: 1.1686994128012884e-07  [2500/7511]\n",
      "loss: 1.0258605698254541e-07  [3000/7511]\n",
      "loss: 8.858246758336463e-08  [3500/7511]\n",
      "loss: 6.172128053094639e-08  [4000/7511]\n",
      "loss: 8.877955082198241e-08  [4500/7511]\n",
      "loss: 8.027933517951169e-08  [5000/7511]\n",
      "loss: 2.6954003828905115e-07  [5500/7511]\n",
      "loss: 1.5542168796400802e-07  [6000/7511]\n",
      "loss: 9.005794510130727e-08  [6500/7511]\n",
      "loss: 3.2294431662194256e-07  [7000/7511]\n",
      "loss: 4.1857896349029033e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.792431678789517e-07 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 6.05587260338325e-08  [0/7511]\n",
      "loss: 1.372511064801074e-07  [500/7511]\n",
      "loss: 2.390032989296742e-07  [1000/7511]\n",
      "loss: 3.3379023989255074e-07  [1500/7511]\n",
      "loss: 7.830855963675276e-08  [2000/7511]\n",
      "loss: 1.282237320765489e-07  [2500/7511]\n",
      "loss: 1.2126214699037519e-07  [3000/7511]\n",
      "loss: 9.592408645175965e-08  [3500/7511]\n",
      "loss: 7.117459404071269e-07  [4000/7511]\n",
      "loss: 2.782531964840018e-06  [4500/7511]\n",
      "loss: 7.939317470118112e-08  [5000/7511]\n",
      "loss: 1.536410536573385e-07  [5500/7511]\n",
      "loss: 9.429556939721806e-07  [6000/7511]\n",
      "loss: 4.673477178585017e-07  [6500/7511]\n",
      "loss: 1.226255648134611e-07  [7000/7511]\n",
      "loss: 2.2016637046817777e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.89576784825213e-07 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 5.561056468650349e-07  [0/7511]\n",
      "loss: 1.010603511986119e-07  [500/7511]\n",
      "loss: 4.329202056396753e-06  [1000/7511]\n",
      "loss: 1.0673056749510579e-07  [1500/7511]\n",
      "loss: 8.8895085070817e-08  [2000/7511]\n",
      "loss: 8.948237564254669e-07  [2500/7511]\n",
      "loss: 1.4898446352162864e-06  [3000/7511]\n",
      "loss: 2.4346340978809167e-06  [3500/7511]\n",
      "loss: 4.709175982497982e-07  [4000/7511]\n",
      "loss: 9.117025712157556e-08  [4500/7511]\n",
      "loss: 4.155365331826033e-06  [5000/7511]\n",
      "loss: 1.0511045047678635e-06  [5500/7511]\n",
      "loss: 2.524684532545507e-07  [6000/7511]\n",
      "loss: 3.662297842765838e-07  [6500/7511]\n",
      "loss: 7.002245894227599e-08  [7000/7511]\n",
      "loss: 5.582103881351941e-07  [7500/7511]\n",
      "Test Error: Avg loss: 4.844662531820694e-07 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# splitting into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=0.3, random_state=30)\n",
    "cols = np.array(X.columns)\n",
    "\n",
    "torch.manual_seed(237943)\n",
    "# dataset class for feeding in data\n",
    "class AirQualityDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, X_arr, y_arr):\n",
    "    self.x_data = torch.tensor(X_arr, \\\n",
    "      dtype=torch.float32)\n",
    "    self.y_data = torch.tensor(y_arr, \\\n",
    "      dtype=torch.float32)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    preds = self.x_data[idx,:]  # or just [idx]\n",
    "    conc = self.y_data[idx] \n",
    "    return (preds, conc)       # tuple of matrices\n",
    "\n",
    "# prepping data for training\n",
    "batch_size = 5\n",
    "train_ds = AirQualityDataset(X_train, y_train)\n",
    "test_ds = AirQualityDataset(X_test, y_test)\n",
    "train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_ldr = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# network architecture\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_tanh_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(82, 60),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(60, 40),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(40, 30),\n",
    "            torch.nn.Tanh(),\n",
    "#             torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 20),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20,10),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(10, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred_conc = self.linear_tanh_stack(x)\n",
    "        return pred_conc\n",
    "    \n",
    "# creating model instance\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "    \n",
    "# initialising hyperparameters\n",
    "learning_rate = 1e-2\n",
    "epochs = 30\n",
    "\n",
    "# initializing the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# initializing the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# defining train and test loops\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss}  [{current}/{size}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: Avg loss: {test_loss} \\n\")\n",
    "    return test_loss\n",
    "    \n",
    "\n",
    "# executing training and testing\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_ldr, model, loss_fn, optimizer)\n",
    "    avg_loss = test_loop(test_ldr, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "model_scores[\"Neural Regression\"] = -avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) Model evaluation / Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_scores_df = pd.DataFrame.from_dict(model_scores, orient = 'index')\n",
    "model_scores_df.columns = [\"Neg. Mean Squared Error\"]\n",
    "plt.figure(figsize = (10, 7))\n",
    "ax = model_scores_df[\"Neg. Mean Squared Error\"].sort_values(ascending=False).plot(kind = \"barh\", title = \"Comparing Model Performance\")\n",
    "fig = ax.get_figure()\n",
    "axes = plt.gca()\n",
    "plt.yticks(fontsize = 10)\n",
    "#plt.show()\n",
    "fig.savefig('model_comparison.jpg', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D) Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually creating model_scores dict\n",
    "model_scores = {}\n",
    "model_scores[\"Random Forest\"] = -3.554107593003395e-08\n",
    "model_scores[\"Extreme Gradient Boosting\"] = -8.322085370215042e-08\n",
    "model_scores[\"Lasso\"] = -2.65918672898003e-07\n",
    "model_scores[\"Ridge\"] = -1.8350045158023289e-07\n",
    "model_scores[\"Linear\"] = -1.7429402374949595e-07\n",
    "model_scores[\"Linear(PCA)\"] = -3.013181658827342e-07\n",
    "#model_scores[\"Neural Regression\"] = -4.3356190018424304e-07  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X_arr, y_arr, cols):\n",
    "    \n",
    "    ## determining optimum no of features\n",
    "    \n",
    "    nof_list=np.arange(1,len(cols))            \n",
    "    neg_score=float(np.inf)\n",
    "    nof=0           \n",
    "    \n",
    "    for n in range(len(nof_list)):\n",
    "        model = LinearRegression()\n",
    "        rfe = RFE(model,nof_list[n])\n",
    "        X_rfe = rfe.fit_transform(X_arr,y_arr)\n",
    "        cv_score_list = cross_val_score(model, X_rfe, \n",
    "                                        y_arr, cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        if(cv_score_list.mean() < neg_score):\n",
    "            neg_score = cv_score_list.mean()\n",
    "            nof = nof_list[n]\n",
    "            \n",
    "    ## getting list of optimum features\n",
    "    \n",
    "    model = LinearRegression()\n",
    "\n",
    "    # initializing RFE model\n",
    "    rfe = RFE(model, nof)    \n",
    "\n",
    "    # transforming data using RFE\n",
    "    X_rfe = rfe.fit_transform(X_arr,y_arr)\n",
    "\n",
    "    # fitting the data to model\n",
    "    model.fit(X_rfe, y_arr)\n",
    "    \n",
    "    # saving model to file\n",
    "    pkl_filename = \"linear_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    temp = pd.Series(rfe.support_, index = cols)\n",
    "    selected_features_rfe = temp[temp==True].index\n",
    "\n",
    "    print(\"Optimum number of features: %d\" %nof)\n",
    "    print(\"Selected features:\", selected_features_rfe)\n",
    "    print(\"Score with %d features: %f\" % (nof, neg_score))\n",
    "    \n",
    "    return neg_score\n",
    "\n",
    "\n",
    "# building the model\n",
    "final_score = linear_model(X_arr, y_arr, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Linear RFE\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection via Filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = df_merged.corr()\n",
    "sns.heatmap(cor, annot=False, cmap=plt.cm.Reds)\n",
    "plt.show()\n",
    "\n",
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"value\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[(cor_target>0.2) & (cor_target != 1.0)]  # anything above 0.2 gives nothing\n",
    "relevant_features\n",
    "\n",
    "# removing features with high levels of multicollinearity\n",
    "selected_features_corr = df_merged[list(relevant_features.index)].corr()\n",
    "\n",
    "# identifying all the features that have a correlation higher than 0.90 or lower than -0.90 indicating a strong positive or negative correlation\n",
    "\n",
    "threshold_1 = 0.90\n",
    "threshold_2 = -0.90\n",
    "\n",
    "def features_high_corr(df_features_corr):\n",
    "    columns = np.full((df_features_corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(df_features_corr.shape[0]):\n",
    "        for j in range(i+1, df_features_corr.shape[0]):\n",
    "            if (df_features_corr.iloc[i,j] >= threshold_1) | (df_features_corr.iloc[i,j] <= threshold_2) :\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    selected_columns = df_features_corr.columns[columns]\n",
    "    return selected_columns\n",
    "\n",
    "# list of features that are not highly correlated\n",
    "selected_features = features_high_corr(selected_features_corr)\n",
    "print(\"Features in dataset that are not highly correlated: \")\n",
    "print(selected_features)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
