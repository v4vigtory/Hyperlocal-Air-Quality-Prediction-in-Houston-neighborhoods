{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model is an OLS model with feature selction being done through a filter method (univariate).Our results indicate that minimum temperature, precipitation, and number of intersections are the most important features to predict air quality.\n",
    "\n",
    "The code chunks below consists of all other models we developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import basic python packages for data analysis and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.lines as mlines\n",
    "import pylab as plot\n",
    "import matplotlib\n",
    "import random\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import math\n",
    "import time\n",
    "\n",
    "### Import Scipy stats packages\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# import sklearn packages\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(style = 'whitegrid')\n",
    "sns.set_palette('bright')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A) Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all datasets to create master_df\n",
    "\n",
    "root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "df_aq = pd.read_csv(root + \"/data/cleaned/air_quality_NO2.csv\", index_col=0)[['value','latitude', 'longitude']]\n",
    "df_met = pd.read_csv(root + \"/data/cleaned/nO2_met.csv\", index_col=0)\n",
    "df_fac = pd.read_csv(root + \"/data/cleaned/no2_fac_data.csv\", index_col=0)\n",
    "# df_fac.drop(df_fac.columns[df_fac.columns.str.contains('_emsdist')], axis=1, inplace=True)\n",
    "df_traffic = pd.read_csv(root + \"/data/cleaned/intersection_final.csv\", index_col=0)\n",
    "\n",
    "df_m1 = df_aq.merge(df_met, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_m2 = df_m1.merge(df_fac, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_merged = df_m2.merge(df_traffic, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_merged.drop(columns = ['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "X = df_merged.drop(\"value\",1) \n",
    "y = df_merged[\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Modeling - Using Cross Validation and Randomized/Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB -- Code to get list of params for an estimator:\n",
    "#regressor.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scale and transform input data\n",
    "def get_data(X, y):\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "    # scaling the data\n",
    "    feature_scaler = StandardScaler()\n",
    "    X = feature_scaler.fit_transform(X)\n",
    "    return X, y\n",
    "\n",
    "# acquiring transformed data\n",
    "X_arr, y_arr = get_data(X, y)\n",
    "cols = np.array(X.columns)\n",
    "\n",
    "# master list for all model scores\n",
    "model_scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_tuning(X_arr, y_arr, grid_param):\n",
    "    print(\"Tuning model\")\n",
    "    # defining the regressor\n",
    "    regressor = RandomForestRegressor(random_state=0)\n",
    "\n",
    "    # doing randomized search\n",
    "    rd_sr = RandomizedSearchCV(estimator=regressor,\n",
    "                               param_distributions=grid_param,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               cv=10,\n",
    "                               n_jobs=-1)\n",
    "\n",
    "    rd_sr.fit(X_arr, y_arr)\n",
    "\n",
    "    # best estimator and it's components\n",
    "    best_parameters = rd_sr.best_params_\n",
    "    best_est_score = rd_sr.best_score_\n",
    "    feature_imp = rd_sr.best_estimator_.feature_importances_\n",
    "    \n",
    "    return rd_sr, best_parameters, best_est_score, feature_imp\n",
    "    \n",
    "\n",
    "def rf_feat_selection(X_arr, y_arr, grid_param, cols):\n",
    "    rd_sr, best_parameters, best_est_score, feature_imp = rf_tuning(X_arr, y_arr, grid_param)\n",
    "    \n",
    "    print(\"Feature selection\")\n",
    "    # plotting feat imp histogram\n",
    "    pd.Series(feature_imp, index=cols).sort_values().plot(kind='barh')\n",
    "    \n",
    "    # calculating optimum number of features\n",
    "    high_score = float(-np.inf)\n",
    "    \n",
    "    for thresh in np.arange(min(feature_imp) + 0.002, max(feature_imp), 0.002):\n",
    "        # identify the most important features using threshold (use RFECV or top 10 percentile later)\n",
    "        sfm = SelectFromModel(rd_sr.best_estimator_, threshold=thresh, prefit=True)\n",
    "        selected_features = cols[sfm.get_support(indices=True)]\n",
    "        print(\"Number of selected features:\", len(selected_features))\n",
    "\n",
    "        # updated dataset\n",
    "        X_updated = sfm.transform(X_arr)\n",
    "\n",
    "        # train a second regressor on this new dataset with hyperparameters of best estimators above using CV\n",
    "        cv_score_list = cross_val_score(RandomForestRegressor(**best_parameters), X_updated, \n",
    "                                        y_arr, cv=10, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        if cv_score_list.mean() > high_score and len(selected_features) > 10:\n",
    "            high_score = cv_score_list.mean()\n",
    "            final_feat_lst = selected_features\n",
    "            feat_arr = X_updated\n",
    "            \n",
    "    return high_score, final_feat_lst, feat_arr, best_parameters, best_est_score\n",
    "\n",
    "\n",
    "def rf_model(X_arr, y_arr, grid_param, cols):\n",
    "    high_score, final_feat_lst, feat_arr, best_parameters, \\\n",
    "        best_est_score = rf_feat_selection(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "    print(\"Saving model to file\")\n",
    "    # saving the model to file\n",
    "    final_model = RandomForestRegressor(**best_parameters)\n",
    "    final_model.fit(feat_arr, y_arr)\n",
    "    pkl_filename = \"rf_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "\n",
    "    print(\"MSE before feature selection:\", best_est_score)\n",
    "    print(\"MSE after feature selection:\", high_score)\n",
    "    print(\"The final list of features are:\", final_feat_lst)\n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    \n",
    "    return high_score\n",
    "\n",
    "\n",
    "grid_param = {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [10, 20, 30, 40, 50],\n",
    "        #'min_samples_split': [2, 5, 10, 15, 20],\n",
    "        #'min_samples_leaf': [1, 2, 5, 10, 15],\n",
    "        #'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "\n",
    "# building the model\n",
    "final_score = rf_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Random Forest\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def xgb_tuning(X_arr, y_arr, grid_param):\n",
    "    print(\"Tuning XGB model\")\n",
    "    \n",
    "    # defining the regressor\n",
    "    \n",
    "    regressor = XGBRegressor(random_state=0)\n",
    "    \n",
    "    # RandomizedSearch\n",
    "    rd_sr = RandomizedSearchCV(estimator = regressor,\n",
    "                                   param_distributions = grid_param,\n",
    "                                   scoring = 'neg_mean_squared_error',\n",
    "                                   cv = 10,\n",
    "                                   n_jobs = -1\n",
    "                                  )\n",
    "    \n",
    "    rd_sr.fit(X_arr, y_arr)\n",
    "    \n",
    "    # best estimator and its components\n",
    "    best_parameters = rd_sr.best_params_\n",
    "    best_est_score = rd_sr.best_score_\n",
    "    feature_imp = rd_sr.best_estimator_.feature_importances_\n",
    "\n",
    "    return rd_sr, best_parameters, best_est_score, feature_imp\n",
    "\n",
    "def plot_hist(data):\n",
    "    plt.figure(figsize = (20,8))\n",
    "    ax = sns.barplot(y = 'Importance', x = 'Feature', \n",
    "                     hue = 'grouping',  \n",
    "                     data = data[:30],\n",
    "                     dodge = False, palette = 'muted')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.xlabel(\"Feature\", size = 20)\n",
    "    plt.xticks(size = 20)\n",
    "    plt.yticks(size = 16)\n",
    "    plt.ylabel(\"Importance\", size = 20)\n",
    "    plt.title(\"Feature selection using XGBoost and Cross Validation\", size = 20)\n",
    "    plt.show()\n",
    "\n",
    "def add_group(data):\n",
    "    data['grouping'] = \" \"\n",
    "    for index, string in enumerate(data['Feature']):\n",
    "        group = re.findall(r'-(.*?)-', string)\n",
    "        if group:\n",
    "            data.loc[index, 'grouping'] = group[0]\n",
    "        else:\n",
    "            data.loc[index, 'grouping'] = data.loc[index, 'Feature']\n",
    "    \n",
    "def xgb_feat_selection(X_arr, y_arr, grid_param, cols):\n",
    "    \n",
    "    rd_sr, best_parameters, best_est_score, feature_imp = xgb_tuning(\n",
    "        X_arr, y_arr, grid_param)\n",
    "    \n",
    "    print(\"Feature selection\")\n",
    "    # plotting feat imp histogram\n",
    "    sf = pd.Series(feature_imp, index=cols)\n",
    "    df = pd.DataFrame({'Feature':sf.index, 'Importance':sf.values})\n",
    "    df = df.sort_values('Importance', ascending = False).reset_index()\n",
    "    add_group(df)\n",
    "    plot_hist(df)\n",
    "    \n",
    "    # calculating optimum number of features\n",
    "    neg_score = float(-np.inf)\n",
    "    \n",
    "    for thresh in np.arange(min(feature_imp) + 0.002, max(feature_imp), 0.002):\n",
    "        # identify the most important features using threshold (use RFECV or top 10 percentile later)\n",
    "        sfm = SelectFromModel(rd_sr.best_estimator_, threshold=thresh, prefit=True)\n",
    "        selected_features = cols[sfm.get_support(indices=True)]\n",
    "        print(\"Number of selected features:\", len(selected_features))\n",
    "\n",
    "        # updated dataset\n",
    "        X_updated = sfm.transform(X_arr)\n",
    "\n",
    "        # train a second regressor on this new dataset with hyperparameters of best estimators above using CV\n",
    "        cv_score_list = cross_val_score(XGBRegressor(**best_parameters), X_updated, \n",
    "                                        y_arr, cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        if cv_score_list.mean() > neg_score and len(selected_features) > 10:\n",
    "            high_score = cv_score_list.mean()\n",
    "            final_feat_lst = selected_features\n",
    "            feat_arr = X_updated\n",
    "            \n",
    "    return high_score, final_feat_lst, feat_arr, best_parameters, best_est_score\n",
    "\n",
    "def xgb_model(X_arr, y_arr, grid_param, cols):\n",
    "    high_score, final_feat_lst, feat_arr, best_parameters, \\\n",
    "        best_est_score = xgb_feat_selection(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "    print(\"Saving model to file\")\n",
    "    # saving the model to file\n",
    "    final_model = XGBRegressor(**best_parameters)\n",
    "    final_model.fit(feat_arr, y_arr)\n",
    "    pkl_filename = \"xgb_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(final_model, file)\n",
    "\n",
    "    print(\"MSE before feature selection:\", best_est_score)\n",
    "    print(\"MSE after feature selection:\", high_score)\n",
    "    print(\"The final list of features are:\", final_feat_lst)\n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    \n",
    "    return high_score\n",
    "\n",
    "grid_param = {\n",
    "        'n_estimators': range(50, 500, 50),\n",
    "        'max_depth': range(5, 50, 5)\n",
    "    }\n",
    "\n",
    "\n",
    "# building the model\n",
    "final_score = xgb_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Extreme Gradient Boosting\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_model(X_arr, y_arr, grid_param, cols):\n",
    "    # defining the regressor\n",
    "    regressor = Lasso(random_state=1)\n",
    "\n",
    "    # do GridSearchCV\n",
    "    gd_sr = GridSearchCV(estimator=regressor,\n",
    "                        param_grid=grid_param,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=10,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "    gd_sr.fit(X_arr, y_arr)\n",
    "    best_parameters = gd_sr.best_params_\n",
    "    best_score = gd_sr.best_score_\n",
    "    \n",
    "    # feature importance list\n",
    "    coef = pd.Series(gd_sr.best_estimator_.coef_, index = cols)\n",
    "    print(\"Lasso picked \" + str(sum(coef != 0)) + \n",
    "      \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "    \n",
    "    # plotting the selected features\n",
    "    feat_imp = coef[coef != 0].sort_values()\n",
    "    plt.figure(figsize = (10, 15))\n",
    "    feat_imp.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature Importance using Lasso\")\n",
    "    plt.yticks(fontsize = 10)\n",
    "    plt.show()\n",
    "    \n",
    "    # saving the model to file\n",
    "    pkl_filename = \"lasso_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(gd_sr.best_estimator_, file)\n",
    "    \n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    print(\"The Neg. MSE after tuning is:\", best_score)\n",
    "    \n",
    "    return best_score\n",
    "\n",
    "\n",
    "grid_param = {\n",
    "            'alpha':[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "            }\n",
    "\n",
    "# building the model\n",
    "final_score = lasso_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Lasso\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_model(X_arr, y_arr, grid_param, cols):\n",
    "    # defining the regressor\n",
    "    regressor = Ridge(random_state=1)\n",
    "\n",
    "    # do GridSearchCV\n",
    "    gd_sr = GridSearchCV(estimator=regressor,\n",
    "                        param_grid=grid_param,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=10,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "    gd_sr.fit(X_arr, y_arr)\n",
    "    best_parameters = gd_sr.best_params_\n",
    "    best_score = gd_sr.best_score_\n",
    "    \n",
    "    # feature importance list\n",
    "    coef = pd.Series(gd_sr.best_estimator_.coef_, index = cols)\n",
    "    print(\"Ridge picked \" + str(sum(coef != 0)) + \n",
    "      \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "    \n",
    "    # plotting the selected features\n",
    "    feat_imp = coef[coef != 0].sort_values()\n",
    "    plt.figure(figsize = (10, 25))\n",
    "    feat_imp.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature Importance using Ridge\")\n",
    "    plt.yticks(fontsize = 10)\n",
    "    plt.show()\n",
    "    \n",
    "    # saving the model to file\n",
    "    pkl_filename = \"ridge_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(gd_sr.best_estimator_, file)\n",
    "    \n",
    "    print(\"The final set of hyperparameters are:\", best_parameters)\n",
    "    print(\"The Neg. MSE after tuning is:\", best_score)\n",
    "    \n",
    "    return best_score\n",
    "\n",
    "\n",
    "grid_param = {\n",
    "            'alpha':[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "            'solver':['auto', 'svd', 'cholesky', 'sag'] \n",
    "            }\n",
    "\n",
    "# building the model\n",
    "final_score = ridge_model(X_arr, y_arr, grid_param, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Ridge\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Linear regression -- With and without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X_arr, y_arr, cols, pca_flag=False):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_arr, y_arr, \n",
    "                                                    test_size=0.3)\n",
    "    if pca_flag:\n",
    "        pca = PCA(.95)\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "        \n",
    "        # saving model to file\n",
    "        pkl_filename = \"pca.pkl\"\n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(pca, file)\n",
    "        \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = - mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # saving regression model to file\n",
    "    pkl_filename = \"linear_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    print(\"Neg. MSE is:\", score)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# building the linear model without PCA\n",
    "final_score = linear_model(X_arr, y_arr, cols)\n",
    "model_scores[\"Linear\"] = final_score\n",
    "\n",
    "# building the linear model with PCA feature selection\n",
    "final_score = linear_model(X_arr, y_arr, cols, pca_flag=True)\n",
    "model_scores[\"Linear(PCA)\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Neural regression via Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_tanh_stack): Sequential(\n",
      "    (0): Linear(in_features=82, out_features=60, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=60, out_features=40, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=40, out_features=30, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (9): Tanh()\n",
      "    (10): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (11): Tanh()\n",
      "    (12): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (13): Tanh()\n",
      "    (14): Linear(in_features=10, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Layer: linear_tanh_stack.0.weight | Size: torch.Size([60, 82]) | Values : tensor([[ 0.0152, -0.0137, -0.0021,  0.0390,  0.0750,  0.0691,  0.0293,  0.0559,\n",
      "          0.0640, -0.0302,  0.0638, -0.0201,  0.0552, -0.0011, -0.0423,  0.0788,\n",
      "          0.0778, -0.1071,  0.0545, -0.0712,  0.0570, -0.0317, -0.0142, -0.0143,\n",
      "          0.0177, -0.0432, -0.0701, -0.0247,  0.0561, -0.0895,  0.0845,  0.0311,\n",
      "         -0.0665,  0.0039, -0.0987,  0.0736,  0.0594, -0.1059,  0.0352, -0.0237,\n",
      "         -0.0907, -0.0714,  0.0820,  0.0040, -0.1028, -0.0656,  0.0235,  0.0890,\n",
      "          0.0931, -0.0658, -0.0137,  0.0807,  0.0165, -0.0323, -0.0730,  0.1027,\n",
      "          0.0479,  0.0343, -0.0335, -0.0331,  0.0658, -0.0074, -0.0573,  0.0012,\n",
      "          0.0726,  0.0292,  0.0851,  0.0976, -0.0925, -0.0385, -0.0609,  0.1004,\n",
      "          0.0297, -0.0887,  0.0463,  0.0189,  0.0628, -0.0848, -0.0766, -0.0513,\n",
      "         -0.0709, -0.0961],\n",
      "        [ 0.0867, -0.1060, -0.0891,  0.0404,  0.0652, -0.0639, -0.0817,  0.0984,\n",
      "          0.0727, -0.0225, -0.0882, -0.0138,  0.0517, -0.0434,  0.0228, -0.0790,\n",
      "         -0.0485, -0.0023,  0.0352,  0.0735, -0.0226,  0.0871,  0.1087, -0.0930,\n",
      "         -0.0912, -0.0691, -0.0607,  0.0723,  0.1010, -0.0726, -0.0669, -0.0842,\n",
      "         -0.0418, -0.0900,  0.0354,  0.0823, -0.0602, -0.0536,  0.0869,  0.0820,\n",
      "         -0.0288, -0.0027, -0.0079, -0.0412,  0.0111,  0.0262,  0.0324,  0.0739,\n",
      "          0.0606, -0.0752,  0.0316,  0.1054,  0.0715, -0.0066, -0.0235,  0.0785,\n",
      "         -0.0010, -0.0922, -0.0550,  0.0004,  0.0100, -0.0335, -0.0764,  0.0187,\n",
      "         -0.0040,  0.1067, -0.0592,  0.0252, -0.0714, -0.0741,  0.0052, -0.0604,\n",
      "          0.0659, -0.0156,  0.0423, -0.0450, -0.0851, -0.0444,  0.0842, -0.0747,\n",
      "         -0.0474, -0.0492]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.0.bias | Size: torch.Size([60]) | Values : tensor([-0.0765, -0.0196], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.2.weight | Size: torch.Size([60, 60]) | Values : tensor([[-0.1198,  0.0932,  0.0888, -0.0076,  0.0621, -0.0269, -0.0399, -0.0856,\n",
      "         -0.0961,  0.1094, -0.0371, -0.0239, -0.1281, -0.0419, -0.0703, -0.0424,\n",
      "          0.0482, -0.0112, -0.0914,  0.0579,  0.1218, -0.1174, -0.1246, -0.0312,\n",
      "          0.0746,  0.0880, -0.1146, -0.0417, -0.1016,  0.0771,  0.0794,  0.0980,\n",
      "          0.0773, -0.0626, -0.0926,  0.0665,  0.0682,  0.0991, -0.0744, -0.0163,\n",
      "         -0.0282, -0.1200, -0.0060, -0.0840, -0.0181,  0.0959, -0.0851,  0.0852,\n",
      "         -0.1268, -0.0701, -0.0468,  0.0127,  0.0379,  0.0380, -0.0993, -0.0140,\n",
      "         -0.0541,  0.0410,  0.1048,  0.0867],\n",
      "        [ 0.1262, -0.0471,  0.0726,  0.0515, -0.0011, -0.1153,  0.0659, -0.1009,\n",
      "         -0.1056, -0.0238,  0.1254,  0.0592, -0.1215, -0.0283, -0.0592,  0.0087,\n",
      "         -0.1064, -0.1195,  0.1284,  0.1101, -0.0699, -0.0231, -0.0928, -0.0092,\n",
      "          0.0201, -0.0757, -0.0582,  0.0247,  0.1033,  0.0389,  0.1134, -0.1277,\n",
      "         -0.0638,  0.0649,  0.0266, -0.0896, -0.0877,  0.0326,  0.0994,  0.1232,\n",
      "          0.0499, -0.0131, -0.0415, -0.0761, -0.0319, -0.0682, -0.1195,  0.0987,\n",
      "         -0.0090, -0.0499,  0.0918,  0.0674, -0.0467, -0.0720, -0.0325,  0.1261,\n",
      "         -0.1169, -0.0163, -0.0321, -0.0744]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.2.bias | Size: torch.Size([60]) | Values : tensor([-0.0506,  0.0688], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.4.weight | Size: torch.Size([40, 60]) | Values : tensor([[ 0.1063,  0.0538, -0.0064, -0.0680,  0.0868,  0.0399,  0.1143, -0.1230,\n",
      "          0.1059, -0.0046, -0.0571, -0.0130, -0.1136,  0.0823,  0.1235,  0.1134,\n",
      "          0.1082,  0.0383, -0.0966, -0.0951,  0.0154,  0.0040,  0.0808, -0.0931,\n",
      "          0.0077, -0.0376,  0.0991, -0.0085,  0.0202,  0.0565, -0.0388, -0.1063,\n",
      "         -0.0245,  0.0225,  0.0010, -0.1203,  0.0664, -0.0350,  0.0022, -0.0242,\n",
      "         -0.0934,  0.1049,  0.0441,  0.0753,  0.0856,  0.1035,  0.0696, -0.0746,\n",
      "          0.0233, -0.0463,  0.0531, -0.0412, -0.0494,  0.0745,  0.0416,  0.1095,\n",
      "          0.1074, -0.0991, -0.1024, -0.1271],\n",
      "        [-0.0930, -0.0841, -0.0640,  0.0298, -0.0187,  0.0681,  0.0554, -0.0074,\n",
      "          0.0495,  0.0675,  0.0178,  0.0392, -0.0788, -0.1104, -0.1045, -0.0248,\n",
      "         -0.0303,  0.0738, -0.0170,  0.0604,  0.1253, -0.0520, -0.0991,  0.0711,\n",
      "         -0.0302, -0.0355, -0.0850, -0.0286, -0.0927,  0.0523,  0.0079,  0.0249,\n",
      "          0.0329, -0.0023,  0.0008, -0.0485, -0.1259,  0.0881, -0.1097,  0.1182,\n",
      "         -0.0119, -0.1043,  0.0720, -0.0175, -0.0294,  0.0526, -0.0374, -0.1192,\n",
      "         -0.1236, -0.0676, -0.0308, -0.0498,  0.0898,  0.1205,  0.0588, -0.0130,\n",
      "          0.0085,  0.0785, -0.0630, -0.1086]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.4.bias | Size: torch.Size([40]) | Values : tensor([0.0280, 0.1270], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.6.weight | Size: torch.Size([30, 40]) | Values : tensor([[ 0.0979, -0.0793, -0.0742, -0.0535,  0.1253,  0.0548, -0.0761,  0.1314,\n",
      "          0.0670,  0.0120,  0.1148, -0.0672, -0.1121,  0.0559, -0.0465, -0.1188,\n",
      "          0.0825, -0.0951,  0.0534, -0.1527,  0.1392, -0.0123,  0.1503,  0.1351,\n",
      "          0.1106,  0.1259, -0.1103, -0.0895, -0.0330,  0.1006, -0.1538,  0.1379,\n",
      "          0.0946, -0.0645,  0.0648, -0.0651,  0.0846,  0.0279,  0.0187,  0.1542],\n",
      "        [ 0.1395,  0.0370,  0.0203, -0.0764,  0.0201, -0.0529, -0.0085,  0.1329,\n",
      "         -0.0516, -0.1486, -0.0385,  0.0758, -0.0988,  0.0805,  0.0140,  0.1344,\n",
      "          0.0779,  0.0837, -0.0342, -0.0840,  0.0882,  0.1173, -0.1094, -0.0891,\n",
      "          0.0098, -0.0941,  0.1137,  0.0672, -0.0225,  0.0860,  0.0932,  0.0054,\n",
      "          0.1465,  0.1276,  0.0609, -0.0406,  0.0471,  0.1319, -0.0723,  0.0938]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.6.bias | Size: torch.Size([30]) | Values : tensor([-0.0409, -0.0140], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.8.weight | Size: torch.Size([20, 30]) | Values : tensor([[-0.0531, -0.1122, -0.1825, -0.0544,  0.0741, -0.1560,  0.0951,  0.0751,\n",
      "          0.0330, -0.1625, -0.1716, -0.1561, -0.1788, -0.0571,  0.1546, -0.1568,\n",
      "          0.0914,  0.0217, -0.1427, -0.0171,  0.0091, -0.0479, -0.1754,  0.1552,\n",
      "          0.0172, -0.1553,  0.1034,  0.1391, -0.0687,  0.0849],\n",
      "        [-0.0143,  0.0003,  0.1293,  0.1209,  0.0579, -0.0286,  0.0591, -0.0026,\n",
      "         -0.1582, -0.1023,  0.0090,  0.1627, -0.0607, -0.1723, -0.0289,  0.1297,\n",
      "         -0.0600, -0.0793,  0.0221, -0.0789,  0.1208, -0.1633, -0.1816,  0.1788,\n",
      "         -0.1309,  0.1824,  0.0230,  0.0954,  0.1645,  0.0510]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.8.bias | Size: torch.Size([20]) | Values : tensor([ 0.1369, -0.0418], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.10.weight | Size: torch.Size([20, 20]) | Values : tensor([[-0.0755,  0.1130, -0.1862, -0.2113, -0.1189, -0.0052,  0.1447, -0.2195,\n",
      "          0.0590, -0.0477,  0.2209, -0.2102, -0.0448, -0.0112, -0.2044, -0.0499,\n",
      "          0.0347, -0.1820, -0.1311, -0.1485],\n",
      "        [ 0.1420, -0.0690, -0.0351,  0.1783,  0.1692, -0.0949, -0.1380,  0.1309,\n",
      "         -0.1949, -0.1144,  0.1563,  0.1163,  0.0929, -0.1390,  0.0991, -0.2020,\n",
      "         -0.0887,  0.2022, -0.1438,  0.1922]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.10.bias | Size: torch.Size([20]) | Values : tensor([0.1207, 0.1887], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.12.weight | Size: torch.Size([10, 20]) | Values : tensor([[-0.1184, -0.1339,  0.0696,  0.0982,  0.1155, -0.1334, -0.0347, -0.1984,\n",
      "         -0.0897, -0.0640, -0.0574, -0.1083,  0.1838,  0.1744,  0.2219, -0.0418,\n",
      "         -0.0912,  0.1803,  0.0842,  0.0574],\n",
      "        [-0.2080, -0.0117, -0.0346, -0.1257,  0.1722, -0.1115,  0.1998,  0.2199,\n",
      "          0.1110,  0.0931,  0.2067, -0.0219,  0.1642, -0.0860, -0.0303,  0.0199,\n",
      "         -0.0431, -0.1109, -0.0443, -0.2197]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.12.bias | Size: torch.Size([10]) | Values : tensor([ 0.1296, -0.1676], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.14.weight | Size: torch.Size([1, 10]) | Values : tensor([[-0.0121, -0.2544, -0.1656,  0.2796, -0.0564, -0.0174, -0.1579, -0.0858,\n",
      "          0.2132, -0.2312]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_tanh_stack.14.bias | Size: torch.Size([1]) | Values : tensor([0.0774], grad_fn=<SliceBackward>) \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.012219807133078575  [0/7511]\n",
      "loss: 2.6816176159627503e-06  [500/7511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.926787487922411e-07  [1000/7511]\n",
      "loss: 3.0924769589546486e-07  [1500/7511]\n",
      "loss: 8.909094049158739e-07  [2000/7511]\n",
      "loss: 3.649212203526986e-07  [2500/7511]\n",
      "loss: 8.70556050358573e-07  [3000/7511]\n",
      "loss: 1.829576632417229e-07  [3500/7511]\n",
      "loss: 1.184392459663286e-07  [4000/7511]\n",
      "loss: 5.039525774463982e-08  [4500/7511]\n",
      "loss: 7.652581075490161e-07  [5000/7511]\n",
      "loss: 1.0464529509590648e-07  [5500/7511]\n",
      "loss: 2.2787011744185293e-07  [6000/7511]\n",
      "loss: 5.981495831974826e-08  [6500/7511]\n",
      "loss: 7.658117056053015e-07  [7000/7511]\n",
      "loss: 1.099590036801601e-07  [7500/7511]\n",
      "Test Error: Avg loss: 2.748143879236649e-06 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 4.5717094110386824e-08  [0/7511]\n",
      "loss: 5.792010142613435e-07  [500/7511]\n",
      "loss: 1.1820664269635017e-07  [1000/7511]\n",
      "loss: 1.2147319239375065e-06  [1500/7511]\n",
      "loss: 9.737659638631158e-07  [2000/7511]\n",
      "loss: 5.887703480311757e-08  [2500/7511]\n",
      "loss: 1.1405472122305582e-07  [3000/7511]\n",
      "loss: 7.84811646781236e-08  [3500/7511]\n",
      "loss: 1.213671566802077e-06  [4000/7511]\n",
      "loss: 1.3740382200921886e-06  [4500/7511]\n",
      "loss: 9.168510359813808e-07  [5000/7511]\n",
      "loss: 1.2932207482663216e-07  [5500/7511]\n",
      "loss: 1.5764744887292181e-07  [6000/7511]\n",
      "loss: 5.461331602418795e-05  [6500/7511]\n",
      "loss: 8.409789131746948e-08  [7000/7511]\n",
      "loss: 4.755211921292357e-05  [7500/7511]\n",
      "Test Error: Avg loss: 2.472831939509086e-06 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.0408424486740842e-06  [0/7511]\n",
      "loss: 1.3682252131275163e-07  [500/7511]\n",
      "loss: 1.4168188045005081e-06  [1000/7511]\n",
      "loss: 1.0873294087332397e-07  [1500/7511]\n",
      "loss: 8.191369715859764e-07  [2000/7511]\n",
      "loss: 6.937445817811749e-08  [2500/7511]\n",
      "loss: 2.5630481559346663e-07  [3000/7511]\n",
      "loss: 3.842662863462465e-07  [3500/7511]\n",
      "loss: 1.6125957813528657e-07  [4000/7511]\n",
      "loss: 2.8409651804395253e-06  [4500/7511]\n",
      "loss: 1.1456287296596201e-07  [5000/7511]\n",
      "loss: 4.668104520533234e-05  [5500/7511]\n",
      "loss: 1.322116673918572e-07  [6000/7511]\n",
      "loss: 1.025205165205989e-06  [6500/7511]\n",
      "loss: 1.3300973478180822e-07  [7000/7511]\n",
      "loss: 8.931088757435646e-08  [7500/7511]\n",
      "Test Error: Avg loss: 2.3151714646073236e-06 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.912851985252928e-05  [0/7511]\n",
      "loss: 1.258234192391683e-07  [500/7511]\n",
      "loss: 3.5059412084592623e-07  [1000/7511]\n",
      "loss: 1.6570127627346665e-05  [1500/7511]\n",
      "loss: 3.447493384101108e-07  [2000/7511]\n",
      "loss: 1.1633696885837708e-06  [2500/7511]\n",
      "loss: 2.016642497437715e-07  [3000/7511]\n",
      "loss: 1.4671613826067187e-06  [3500/7511]\n",
      "loss: 8.416697028224007e-07  [4000/7511]\n",
      "loss: 1.194637064827475e-07  [4500/7511]\n",
      "loss: 3.103268682025373e-07  [5000/7511]\n",
      "loss: 1.0251785909076716e-07  [5500/7511]\n",
      "loss: 5.2914390835212544e-05  [6000/7511]\n",
      "loss: 5.419305892928605e-08  [6500/7511]\n",
      "loss: 1.9931334804823564e-07  [7000/7511]\n",
      "loss: 2.0879951989627443e-06  [7500/7511]\n",
      "Test Error: Avg loss: 2.1854721972246894e-06 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.338447788157282e-07  [0/7511]\n",
      "loss: 1.2143350431870203e-06  [500/7511]\n",
      "loss: 2.0689499251602683e-06  [1000/7511]\n",
      "loss: 1.3528224940273503e-07  [1500/7511]\n",
      "loss: 2.6881034500547685e-06  [2000/7511]\n",
      "loss: 2.6731077014119364e-06  [2500/7511]\n",
      "loss: 4.907540187559789e-07  [3000/7511]\n",
      "loss: 1.860089469118975e-07  [3500/7511]\n",
      "loss: 2.4003481939871563e-06  [4000/7511]\n",
      "loss: 3.0881489010425867e-07  [4500/7511]\n",
      "loss: 2.0694012903277326e-07  [5000/7511]\n",
      "loss: 6.801141694268154e-07  [5500/7511]\n",
      "loss: 1.8216191222109046e-07  [6000/7511]\n",
      "loss: 1.7189093171054992e-07  [6500/7511]\n",
      "loss: 8.885119200385816e-07  [7000/7511]\n",
      "loss: 1.633031502024096e-06  [7500/7511]\n",
      "Test Error: Avg loss: 2.0983924359361263e-06 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.3896606055350276e-07  [0/7511]\n",
      "loss: 5.483798304339871e-08  [500/7511]\n",
      "loss: 8.416817820489086e-08  [1000/7511]\n",
      "loss: 1.1047952739318134e-06  [1500/7511]\n",
      "loss: 2.9328705863918003e-07  [2000/7511]\n",
      "loss: 7.462338658115186e-08  [2500/7511]\n",
      "loss: 6.341614948723873e-07  [3000/7511]\n",
      "loss: 2.3644570035230572e-07  [3500/7511]\n",
      "loss: 1.0505855385645191e-07  [4000/7511]\n",
      "loss: 9.855356211119215e-07  [4500/7511]\n",
      "loss: 1.7762808113275241e-07  [5000/7511]\n",
      "loss: 1.8716029615006846e-07  [5500/7511]\n",
      "loss: 1.2092625638615573e-06  [6000/7511]\n",
      "loss: 5.119470370118506e-05  [6500/7511]\n",
      "loss: 6.087873316573678e-07  [7000/7511]\n",
      "loss: 7.018294212457477e-08  [7500/7511]\n",
      "Test Error: Avg loss: 2.0583482195185264e-06 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.2893933387658763e-07  [0/7511]\n",
      "loss: 7.098554277717994e-08  [500/7511]\n",
      "loss: 8.268079909612425e-08  [1000/7511]\n",
      "loss: 1.604332311444523e-07  [1500/7511]\n",
      "loss: 1.671222946697526e-07  [2000/7511]\n",
      "loss: 5.419789204097469e-07  [2500/7511]\n",
      "loss: 1.104966642628824e-07  [3000/7511]\n",
      "loss: 5.786929335727109e-08  [3500/7511]\n",
      "loss: 1.753568540152628e-05  [4000/7511]\n",
      "loss: 6.332226689664822e-07  [4500/7511]\n",
      "loss: 1.3076298728265101e-06  [5000/7511]\n",
      "loss: 3.339455645345879e-07  [5500/7511]\n",
      "loss: 1.1495779972392484e-07  [6000/7511]\n",
      "loss: 1.257442505675499e-07  [6500/7511]\n",
      "loss: 5.719130058423616e-07  [7000/7511]\n",
      "loss: 3.004416981866598e-08  [7500/7511]\n",
      "Test Error: Avg loss: 2.0022465076130616e-06 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.1127998504889547e-06  [0/7511]\n",
      "loss: 4.7044727580214385e-07  [500/7511]\n",
      "loss: 3.8351382158907654e-07  [1000/7511]\n",
      "loss: 1.62952346727252e-07  [1500/7511]\n",
      "loss: 5.738920094700006e-07  [2000/7511]\n",
      "loss: 1.5270541098288959e-06  [2500/7511]\n",
      "loss: 3.038490490325785e-07  [3000/7511]\n",
      "loss: 1.822946842366946e-07  [3500/7511]\n",
      "loss: 3.165757505030342e-07  [4000/7511]\n",
      "loss: 3.568612783055869e-07  [4500/7511]\n",
      "loss: 7.712587830610573e-07  [5000/7511]\n",
      "loss: 1.1182187336089555e-06  [5500/7511]\n",
      "loss: 9.647058618611482e-08  [6000/7511]\n",
      "loss: 9.773359010978311e-08  [6500/7511]\n",
      "loss: 1.4766303024771332e-07  [7000/7511]\n",
      "loss: 9.080415708240253e-08  [7500/7511]\n",
      "Test Error: Avg loss: 1.90998658185597e-06 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 6.707732325139659e-08  [0/7511]\n",
      "loss: 1.7759248294169083e-07  [500/7511]\n",
      "loss: 4.14578806839927e-08  [1000/7511]\n",
      "loss: 1.1150352463573654e-07  [1500/7511]\n",
      "loss: 1.962831674973131e-06  [2000/7511]\n",
      "loss: 1.1237929697927029e-07  [2500/7511]\n",
      "loss: 9.083830576628316e-08  [3000/7511]\n",
      "loss: 4.545124170363124e-07  [3500/7511]\n",
      "loss: 1.6994864893149497e-07  [4000/7511]\n",
      "loss: 8.350187385985919e-07  [4500/7511]\n",
      "loss: 1.2153635964295972e-07  [5000/7511]\n",
      "loss: 7.56088752495998e-07  [5500/7511]\n",
      "loss: 1.8108060828581074e-07  [6000/7511]\n",
      "loss: 9.462590355724387e-08  [6500/7511]\n",
      "loss: 1.0041348019740326e-07  [7000/7511]\n",
      "loss: 1.2591300446729292e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.8739231661543486e-06 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 7.354153552796561e-08  [0/7511]\n",
      "loss: 4.210949919070117e-05  [500/7511]\n",
      "loss: 1.8401910040211078e-07  [1000/7511]\n",
      "loss: 1.353323284547514e-07  [1500/7511]\n",
      "loss: 1.990228639670022e-07  [2000/7511]\n",
      "loss: 1.436051206837874e-07  [2500/7511]\n",
      "loss: 7.884558073101289e-08  [3000/7511]\n",
      "loss: 2.241626361865201e-06  [3500/7511]\n",
      "loss: 2.221291424575611e-06  [4000/7511]\n",
      "loss: 1.0359744351262634e-07  [4500/7511]\n",
      "loss: 1.2239445368322777e-06  [5000/7511]\n",
      "loss: 4.796439867504887e-08  [5500/7511]\n",
      "loss: 3.6374007095218985e-07  [6000/7511]\n",
      "loss: 2.3720087938272627e-07  [6500/7511]\n",
      "loss: 9.573374626370423e-08  [7000/7511]\n",
      "loss: 5.0002480378452674e-08  [7500/7511]\n",
      "Test Error: Avg loss: 1.7875398388045995e-06 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 5.240021891950164e-07  [0/7511]\n",
      "loss: 3.5591391167599795e-08  [500/7511]\n",
      "loss: 9.72883754002396e-07  [1000/7511]\n",
      "loss: 1.0375488557201606e-07  [1500/7511]\n",
      "loss: 1.592266052341529e-08  [2000/7511]\n",
      "loss: 9.395812980983465e-07  [2500/7511]\n",
      "loss: 4.891982712251775e-07  [3000/7511]\n",
      "loss: 1.3090881338939653e-06  [3500/7511]\n",
      "loss: 8.828654074477527e-08  [4000/7511]\n",
      "loss: 1.4510607115880703e-07  [4500/7511]\n",
      "loss: 9.740855944073701e-08  [5000/7511]\n",
      "loss: 1.325150265074626e-07  [5500/7511]\n",
      "loss: 2.9463319606293226e-07  [6000/7511]\n",
      "loss: 1.3730995362948306e-07  [6500/7511]\n",
      "loss: 1.5078336446094909e-06  [7000/7511]\n",
      "loss: 2.442435288685374e-06  [7500/7511]\n",
      "Test Error: Avg loss: 1.7791268490380643e-06 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 5.2883427997585386e-05  [0/7511]\n",
      "loss: 4.4948194499738747e-07  [500/7511]\n",
      "loss: 1.6534410463009408e-07  [1000/7511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.313313747341454e-07  [1500/7511]\n",
      "loss: 2.4052205844782293e-06  [2000/7511]\n",
      "loss: 3.7967103594382934e-07  [2500/7511]\n",
      "loss: 1.7265644203234842e-07  [3000/7511]\n",
      "loss: 3.182874934282154e-06  [3500/7511]\n",
      "loss: 1.8559890690994507e-07  [4000/7511]\n",
      "loss: 4.915020781481871e-07  [4500/7511]\n",
      "loss: 5.0816332475278614e-08  [5000/7511]\n",
      "loss: 2.535912528855988e-07  [5500/7511]\n",
      "loss: 3.676620679016196e-07  [6000/7511]\n",
      "loss: 9.795657263111934e-08  [6500/7511]\n",
      "loss: 2.054494643743965e-06  [7000/7511]\n",
      "loss: 2.096461457767873e-06  [7500/7511]\n",
      "Test Error: Avg loss: 1.7015938289634881e-06 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 6.321953804899749e-08  [0/7511]\n",
      "loss: 1.1598192486417247e-06  [500/7511]\n",
      "loss: 1.8497812561690807e-06  [1000/7511]\n",
      "loss: 5.10327311076253e-07  [1500/7511]\n",
      "loss: 7.521818190525664e-08  [2000/7511]\n",
      "loss: 1.5390259022751707e-07  [2500/7511]\n",
      "loss: 3.095057365953835e-07  [3000/7511]\n",
      "loss: 3.048871022315325e-08  [3500/7511]\n",
      "loss: 3.253593945373723e-07  [4000/7511]\n",
      "loss: 1.452663695999945e-07  [4500/7511]\n",
      "loss: 2.3352140487986617e-06  [5000/7511]\n",
      "loss: 1.0133672390111315e-07  [5500/7511]\n",
      "loss: 2.4632283768255547e-08  [6000/7511]\n",
      "loss: 2.5899339561874513e-06  [6500/7511]\n",
      "loss: 1.1929427046197816e-07  [7000/7511]\n",
      "loss: 7.974245619379872e-08  [7500/7511]\n",
      "Test Error: Avg loss: 1.6426564870015179e-06 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.4799660164044326e-07  [0/7511]\n",
      "loss: 1.550582169329573e-07  [500/7511]\n",
      "loss: 1.2705405083579535e-07  [1000/7511]\n",
      "loss: 8.599888019489299e-07  [1500/7511]\n",
      "loss: 4.656504515310189e-08  [2000/7511]\n",
      "loss: 1.2047768223055755e-06  [2500/7511]\n",
      "loss: 4.8417787184007466e-05  [3000/7511]\n",
      "loss: 9.684521273811697e-07  [3500/7511]\n",
      "loss: 1.5729598601410544e-07  [4000/7511]\n",
      "loss: 8.397498874046505e-08  [4500/7511]\n",
      "loss: 6.275841428760032e-07  [5000/7511]\n",
      "loss: 2.995843146891275e-07  [5500/7511]\n",
      "loss: 8.42025713154726e-07  [6000/7511]\n",
      "loss: 8.721289646018704e-08  [6500/7511]\n",
      "loss: 1.8051625261250592e-07  [7000/7511]\n",
      "loss: 4.7117063672885706e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.6129703747747094e-06 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 7.605129326293536e-08  [0/7511]\n",
      "loss: 7.467215823453444e-08  [500/7511]\n",
      "loss: 1.7497468718374876e-07  [1000/7511]\n",
      "loss: 1.8710315430325863e-07  [1500/7511]\n",
      "loss: 2.3700726714537268e-08  [2000/7511]\n",
      "loss: 1.0612959613354178e-06  [2500/7511]\n",
      "loss: 1.4367561789185856e-06  [3000/7511]\n",
      "loss: 7.370090884251113e-07  [3500/7511]\n",
      "loss: 1.3233915296950727e-07  [4000/7511]\n",
      "loss: 2.6939444524032297e-06  [4500/7511]\n",
      "loss: 6.20068760781578e-08  [5000/7511]\n",
      "loss: 1.5932321275613504e-06  [5500/7511]\n",
      "loss: 2.2166004498558323e-07  [6000/7511]\n",
      "loss: 1.732450556346521e-07  [6500/7511]\n",
      "loss: 1.575308630208383e-07  [7000/7511]\n",
      "loss: 4.117927403513022e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.5776461984660454e-06 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.7928843476511247e-07  [0/7511]\n",
      "loss: 9.720451998873614e-08  [500/7511]\n",
      "loss: 1.2333194376878964e-07  [1000/7511]\n",
      "loss: 7.711874161486776e-08  [1500/7511]\n",
      "loss: 1.2711576857782347e-07  [2000/7511]\n",
      "loss: 5.871709163329797e-07  [2500/7511]\n",
      "loss: 9.473049544794776e-07  [3000/7511]\n",
      "loss: 1.6405284668508102e-06  [3500/7511]\n",
      "loss: 9.844566193351056e-07  [4000/7511]\n",
      "loss: 3.462907329776499e-07  [4500/7511]\n",
      "loss: 2.1466479438458919e-07  [5000/7511]\n",
      "loss: 8.101376636204805e-08  [5500/7511]\n",
      "loss: 1.5508751616266636e-08  [6000/7511]\n",
      "loss: 4.426410171731732e-08  [6500/7511]\n",
      "loss: 1.683098673765926e-07  [7000/7511]\n",
      "loss: 3.5669589237841137e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.5343896844252378e-06 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.86165715124298e-07  [0/7511]\n",
      "loss: 7.624114459758857e-07  [500/7511]\n",
      "loss: 1.4322210972750327e-07  [1000/7511]\n",
      "loss: 7.451658490253976e-08  [1500/7511]\n",
      "loss: 9.065465178537124e-08  [2000/7511]\n",
      "loss: 5.120542937220307e-06  [2500/7511]\n",
      "loss: 7.003244490988436e-07  [3000/7511]\n",
      "loss: 7.92647085745557e-08  [3500/7511]\n",
      "loss: 2.5618996915000025e-06  [4000/7511]\n",
      "loss: 6.127473852757248e-07  [4500/7511]\n",
      "loss: 9.296023222304939e-07  [5000/7511]\n",
      "loss: 7.278667339960521e-07  [5500/7511]\n",
      "loss: 2.793914859466895e-07  [6000/7511]\n",
      "loss: 9.283246527047595e-07  [6500/7511]\n",
      "loss: 7.334168117267836e-07  [7000/7511]\n",
      "loss: 7.435823499690741e-05  [7500/7511]\n",
      "Test Error: Avg loss: 1.5977931776138311e-06 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 9.54399638430914e-07  [0/7511]\n",
      "loss: 1.8714916905082646e-07  [500/7511]\n",
      "loss: 2.0529517996692448e-07  [1000/7511]\n",
      "loss: 1.7065120516690513e-07  [1500/7511]\n",
      "loss: 5.838287364667849e-08  [2000/7511]\n",
      "loss: 2.3197810605779523e-07  [2500/7511]\n",
      "loss: 1.171179860648408e-06  [3000/7511]\n",
      "loss: 2.0949225927324733e-06  [3500/7511]\n",
      "loss: 2.3747163595544407e-06  [4000/7511]\n",
      "loss: 1.1250869391687957e-07  [4500/7511]\n",
      "loss: 7.068265972520749e-08  [5000/7511]\n",
      "loss: 1.1011617289113929e-06  [5500/7511]\n",
      "loss: 2.3090571232842194e-07  [6000/7511]\n",
      "loss: 4.5909357140772045e-05  [6500/7511]\n",
      "loss: 5.597069616669614e-07  [7000/7511]\n",
      "loss: 1.282836706195667e-06  [7500/7511]\n",
      "Test Error: Avg loss: 1.4884248978534164e-06 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.4422185163075483e-08  [0/7511]\n",
      "loss: 1.199227739334674e-07  [500/7511]\n",
      "loss: 8.342401969230195e-08  [1000/7511]\n",
      "loss: 1.3862077139492612e-06  [1500/7511]\n",
      "loss: 7.111584920949099e-08  [2000/7511]\n",
      "loss: 1.4318794683276792e-07  [2500/7511]\n",
      "loss: 3.240227215428604e-06  [3000/7511]\n",
      "loss: 4.772244110995416e-08  [3500/7511]\n",
      "loss: 4.300152411929048e-08  [4000/7511]\n",
      "loss: 7.460214135335264e-08  [4500/7511]\n",
      "loss: 7.64014558285453e-08  [5000/7511]\n",
      "loss: 2.6733134745882126e-06  [5500/7511]\n",
      "loss: 2.689514246867475e-07  [6000/7511]\n",
      "loss: 1.6666034525769646e-06  [6500/7511]\n",
      "loss: 1.2397464388413937e-07  [7000/7511]\n",
      "loss: 4.786967977565837e-08  [7500/7511]\n",
      "Test Error: Avg loss: 1.4636386494966704e-06 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 1.0665030458767433e-07  [0/7511]\n",
      "loss: 1.1120949920950807e-06  [500/7511]\n",
      "loss: 9.468693207281831e-08  [1000/7511]\n",
      "loss: 9.830828275880776e-07  [1500/7511]\n",
      "loss: 3.1344569606517325e-07  [2000/7511]\n",
      "loss: 3.611106791368002e-08  [2500/7511]\n",
      "loss: 1.1101782320110942e-06  [3000/7511]\n",
      "loss: 1.7431817411761585e-07  [3500/7511]\n",
      "loss: 2.71793140882437e-07  [4000/7511]\n",
      "loss: 1.5952520016071503e-06  [4500/7511]\n",
      "loss: 7.352420539064042e-07  [5000/7511]\n",
      "loss: 9.931500244420022e-08  [5500/7511]\n",
      "loss: 7.961068604345201e-07  [6000/7511]\n",
      "loss: 1.1663615850920905e-07  [6500/7511]\n",
      "loss: 7.966270914039342e-07  [7000/7511]\n",
      "loss: 1.0852112808379388e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.4083467410080758e-06 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 7.342845975699674e-08  [0/7511]\n",
      "loss: 2.7387582690607815e-07  [500/7511]\n",
      "loss: 6.137964447816557e-08  [1000/7511]\n",
      "loss: 7.86930272056452e-08  [1500/7511]\n",
      "loss: 1.5055923086038092e-07  [2000/7511]\n",
      "loss: 6.728139823053425e-08  [2500/7511]\n",
      "loss: 1.7394462759057205e-07  [3000/7511]\n",
      "loss: 1.1897746787781216e-07  [3500/7511]\n",
      "loss: 1.4378986179508502e-07  [4000/7511]\n",
      "loss: 2.094024296184216e-07  [4500/7511]\n",
      "loss: 7.15529154149408e-07  [5000/7511]\n",
      "loss: 2.1066426825200324e-07  [5500/7511]\n",
      "loss: 5.603821477961901e-07  [6000/7511]\n",
      "loss: 3.835437780708162e-07  [6500/7511]\n",
      "loss: 1.7699589704989194e-07  [7000/7511]\n",
      "loss: 5.457870884129079e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.3558939473653933e-06 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 1.2706290419828292e-07  [0/7511]\n",
      "loss: 1.5485383642044326e-07  [500/7511]\n",
      "loss: 9.142578960563696e-08  [1000/7511]\n",
      "loss: 1.3590788512374274e-06  [1500/7511]\n",
      "loss: 2.4202333293033007e-07  [2000/7511]\n",
      "loss: 8.213857682903836e-08  [2500/7511]\n",
      "loss: 1.6274346990030608e-06  [3000/7511]\n",
      "loss: 1.272952800945859e-07  [3500/7511]\n",
      "loss: 5.588068461293005e-07  [4000/7511]\n",
      "loss: 6.648198791481263e-07  [4500/7511]\n",
      "loss: 2.149307505305842e-07  [5000/7511]\n",
      "loss: 9.70183918980183e-07  [5500/7511]\n",
      "loss: 3.3065313687075104e-07  [6000/7511]\n",
      "loss: 7.673039448263808e-08  [6500/7511]\n",
      "loss: 3.8204321754164994e-05  [7000/7511]\n",
      "loss: 2.397300988832285e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.326728951683256e-06 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.6139949821081245e-07  [0/7511]\n",
      "loss: 5.4263718851643716e-08  [500/7511]\n",
      "loss: 9.10797552933218e-07  [1000/7511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.122112037905026e-07  [1500/7511]\n",
      "loss: 1.756445158207498e-06  [2000/7511]\n",
      "loss: 1.0052415433392525e-07  [2500/7511]\n",
      "loss: 1.508798419536106e-07  [3000/7511]\n",
      "loss: 5.092723768029828e-06  [3500/7511]\n",
      "loss: 2.3748443709337153e-06  [4000/7511]\n",
      "loss: 1.1793262189030429e-07  [4500/7511]\n",
      "loss: 1.0506022363188094e-07  [5000/7511]\n",
      "loss: 1.1662467613859917e-07  [5500/7511]\n",
      "loss: 9.759436636613827e-08  [6000/7511]\n",
      "loss: 1.8801059695761069e-06  [6500/7511]\n",
      "loss: 6.719009206790361e-07  [7000/7511]\n",
      "loss: 6.726995138706116e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.3134425784711367e-06 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 1.4027580164110987e-07  [0/7511]\n",
      "loss: 2.342274001421174e-06  [500/7511]\n",
      "loss: 1.2191517271276098e-06  [1000/7511]\n",
      "loss: 6.780324213195854e-08  [1500/7511]\n",
      "loss: 2.2919618913874729e-07  [2000/7511]\n",
      "loss: 4.954670203005662e-07  [2500/7511]\n",
      "loss: 3.0259525374276564e-07  [3000/7511]\n",
      "loss: 2.9637601528520463e-07  [3500/7511]\n",
      "loss: 1.2310181318753166e-07  [4000/7511]\n",
      "loss: 1.620003473590259e-07  [4500/7511]\n",
      "loss: 1.3499460749244463e-07  [5000/7511]\n",
      "loss: 3.2985042253130814e-06  [5500/7511]\n",
      "loss: 2.802454446282354e-06  [6000/7511]\n",
      "loss: 5.204718789286744e-08  [6500/7511]\n",
      "loss: 1.6767771171544155e-07  [7000/7511]\n",
      "loss: 4.749972504214384e-06  [7500/7511]\n",
      "Test Error: Avg loss: 1.2637445768330414e-06 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 4.9062300888635946e-08  [0/7511]\n",
      "loss: 1.0950440127999173e-06  [500/7511]\n",
      "loss: 2.633184408296074e-07  [1000/7511]\n",
      "loss: 1.560896635055542e-06  [1500/7511]\n",
      "loss: 7.069168361795164e-08  [2000/7511]\n",
      "loss: 6.611435878767224e-07  [2500/7511]\n",
      "loss: 1.4539186565798445e-07  [3000/7511]\n",
      "loss: 2.9712762170674978e-06  [3500/7511]\n",
      "loss: 5.176460859956933e-08  [4000/7511]\n",
      "loss: 2.971387687011884e-07  [4500/7511]\n",
      "loss: 7.081167296973945e-08  [5000/7511]\n",
      "loss: 2.9750053727184422e-06  [5500/7511]\n",
      "loss: 7.211097852177772e-08  [6000/7511]\n",
      "loss: 2.049968088613241e-07  [6500/7511]\n",
      "loss: 6.669213803434104e-08  [7000/7511]\n",
      "loss: 1.6084563014828746e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.2388682248540094e-06 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.6342465275774885e-07  [0/7511]\n",
      "loss: 8.517114338246756e-07  [500/7511]\n",
      "loss: 1.877593149401946e-06  [1000/7511]\n",
      "loss: 1.2549384109661332e-07  [1500/7511]\n",
      "loss: 1.4957780081203964e-07  [2000/7511]\n",
      "loss: 2.826339482453477e-07  [2500/7511]\n",
      "loss: 2.1102052869537147e-06  [3000/7511]\n",
      "loss: 9.717066404846264e-07  [3500/7511]\n",
      "loss: 9.783256871287449e-08  [4000/7511]\n",
      "loss: 9.491339625355977e-08  [4500/7511]\n",
      "loss: 2.7747782382903097e-07  [5000/7511]\n",
      "loss: 1.3745202522841282e-06  [5500/7511]\n",
      "loss: 1.4492619584416389e-06  [6000/7511]\n",
      "loss: 5.113307466331207e-08  [6500/7511]\n",
      "loss: 5.319871547726507e-07  [7000/7511]\n",
      "loss: 2.7225112830819853e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.212706314356816e-06 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 1.3512733403331367e-06  [0/7511]\n",
      "loss: 1.2580180452914647e-07  [500/7511]\n",
      "loss: 1.1657272125376039e-07  [1000/7511]\n",
      "loss: 4.682371468334168e-08  [1500/7511]\n",
      "loss: 6.85527425048349e-07  [2000/7511]\n",
      "loss: 3.150023530906765e-07  [2500/7511]\n",
      "loss: 8.038509804464411e-07  [3000/7511]\n",
      "loss: 1.3871390081021673e-07  [3500/7511]\n",
      "loss: 1.1040787484262182e-07  [4000/7511]\n",
      "loss: 1.8367948939612688e-07  [4500/7511]\n",
      "loss: 5.7241635431637405e-08  [5000/7511]\n",
      "loss: 1.4547816817866988e-07  [5500/7511]\n",
      "loss: 9.425141200836151e-08  [6000/7511]\n",
      "loss: 1.2852935071805405e-07  [6500/7511]\n",
      "loss: 1.1583026093830995e-07  [7000/7511]\n",
      "loss: 9.693120972542602e-08  [7500/7511]\n",
      "Test Error: Avg loss: 1.1827033859887085e-06 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.095054699362663e-06  [0/7511]\n",
      "loss: 7.06035109487857e-07  [500/7511]\n",
      "loss: 1.280635899547633e-07  [1000/7511]\n",
      "loss: 6.454823164858681e-08  [1500/7511]\n",
      "loss: 8.217163838253327e-08  [2000/7511]\n",
      "loss: 8.337552515058633e-08  [2500/7511]\n",
      "loss: 2.2133474431029754e-07  [3000/7511]\n",
      "loss: 2.175981535401661e-06  [3500/7511]\n",
      "loss: 1.1197544154128991e-06  [4000/7511]\n",
      "loss: 2.9777561394439545e-07  [4500/7511]\n",
      "loss: 1.0356682196288602e-06  [5000/7511]\n",
      "loss: 3.2640098197589396e-06  [5500/7511]\n",
      "loss: 8.412467877860763e-08  [6000/7511]\n",
      "loss: 9.216771701403559e-08  [6500/7511]\n",
      "loss: 1.1522462983748483e-07  [7000/7511]\n",
      "loss: 1.0150268536790463e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.1586186224662957e-06 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.63094420915877e-07  [0/7511]\n",
      "loss: 4.1411061602047994e-07  [500/7511]\n",
      "loss: 2.6641032491170336e-08  [1000/7511]\n",
      "loss: 1.2521017822564318e-07  [1500/7511]\n",
      "loss: 1.4802088799115154e-07  [2000/7511]\n",
      "loss: 5.296432163959253e-07  [2500/7511]\n",
      "loss: 7.655409461904128e-08  [3000/7511]\n",
      "loss: 8.884084934379644e-08  [3500/7511]\n",
      "loss: 1.528539712580823e-07  [4000/7511]\n",
      "loss: 3.5190868175050127e-07  [4500/7511]\n",
      "loss: 1.5907769466139143e-06  [5000/7511]\n",
      "loss: 1.1349642647928704e-07  [5500/7511]\n",
      "loss: 4.133841571274388e-07  [6000/7511]\n",
      "loss: 1.4867991637856903e-07  [6500/7511]\n",
      "loss: 4.6600280967368235e-08  [7000/7511]\n",
      "loss: 1.6607275199476135e-07  [7500/7511]\n",
      "Test Error: Avg loss: 1.1532582298079064e-06 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 8.83109294136375e-07  [0/7511]\n",
      "loss: 1.0578276743444803e-07  [500/7511]\n",
      "loss: 7.743867769249846e-08  [1000/7511]\n",
      "loss: 9.093668040804914e-07  [1500/7511]\n",
      "loss: 1.321774016105337e-05  [2000/7511]\n",
      "loss: 5.998445118393647e-08  [2500/7511]\n",
      "loss: 1.4810950688115554e-07  [3000/7511]\n",
      "loss: 3.3577248359506484e-07  [3500/7511]\n",
      "loss: 1.4033251716227824e-07  [4000/7511]\n",
      "loss: 8.16499081679467e-08  [4500/7511]\n",
      "loss: 1.786769132650079e-07  [5000/7511]\n",
      "loss: 2.2240847386001406e-07  [5500/7511]\n",
      "loss: 2.479245040376554e-07  [6000/7511]\n",
      "loss: 1.4869746109980042e-06  [6500/7511]\n",
      "loss: 2.3168264817741147e-07  [7000/7511]\n",
      "loss: 1.2489638265833491e-06  [7500/7511]\n",
      "Test Error: Avg loss: 1.1264318157377733e-06 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# splitting into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=0.3)\n",
    "cols = np.array(X.columns)\n",
    "\n",
    "# dataset class for feeding in data\n",
    "class AirQualityDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, X_arr, y_arr):\n",
    "    self.x_data = torch.tensor(X_arr, \\\n",
    "      dtype=torch.float32)\n",
    "    self.y_data = torch.tensor(y_arr, \\\n",
    "      dtype=torch.float32)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    preds = self.x_data[idx,:]  # or just [idx]\n",
    "    conc = self.y_data[idx] \n",
    "    return (preds, conc)       # tuple of matrices\n",
    "\n",
    "# prepping data for training\n",
    "batch_size = 5\n",
    "train_ds = AirQualityDataset(X_train, y_train)\n",
    "test_ds = AirQualityDataset(X_test, y_test)\n",
    "train_ldr = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_ldr = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# network architecture\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_tanh_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(82, 60),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(60, 60),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(60, 40),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(40, 30),\n",
    "            torch.nn.Tanh(),\n",
    "#             torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 20),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20,10),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(10, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred_conc = self.linear_tanh_stack(x)\n",
    "        return pred_conc\n",
    "    \n",
    "# creating model instance\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "    \n",
    "# initialising hyperparameters\n",
    "learning_rate = 1e-2\n",
    "epochs = 30\n",
    "\n",
    "# initializing the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# initializing the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# defining train and test loops\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss}  [{current}/{size}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: Avg loss: {test_loss} \\n\")\n",
    "    return test_loss\n",
    "    \n",
    "\n",
    "# executing training and testing\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_ldr, model, loss_fn, optimizer)\n",
    "    avg_loss = test_loop(test_ldr, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "model_scores[\"Neural Regression\"] = -avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C) Model evaluation / Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores_df = pd.DataFrame.from_dict(model_scores, orient = 'index')\n",
    "model_scores_df.columns = [\"Neg. Mean Squared Error\"]\n",
    "plt.figure(figsize = (10, 7))\n",
    "ax = model_scores_df[\"Neg. Mean Squared Error\"].sort_values(ascending=False).plot(kind = \"barh\", title = \"Comparing Model Performance\")\n",
    "fig = ax.get_figure()\n",
    "axes = plt.gca()\n",
    "plt.yticks(fontsize = 10)\n",
    "#plt.show()\n",
    "fig.savefig('model_comparison.jpg', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (D) Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually creating model_scores dict\n",
    "model_scores = {}\n",
    "model_scores[\"Random Forest\"] = -3.554107593003395e-08\n",
    "model_scores[\"Extreme Gradient Boosting\"] = -8.322085370215042e-08\n",
    "model_scores[\"Lasso\"] = -2.65918672898003e-07\n",
    "model_scores[\"Ridge\"] = -1.8350045158023289e-07\n",
    "model_scores[\"Linear\"] = -1.7429402374949595e-07\n",
    "model_scores[\"Linear(PCA)\"] = -3.013181658827342e-07\n",
    "#model_scores[\"Neural Regression\"] = -2.0e-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X_arr, y_arr, cols):\n",
    "    \n",
    "    ## determining optimum no of features\n",
    "    \n",
    "    nof_list=np.arange(1,len(cols))            \n",
    "    neg_score=float(np.inf)\n",
    "    nof=0           \n",
    "    \n",
    "    for n in range(len(nof_list)):\n",
    "        model = LinearRegression()\n",
    "        rfe = RFE(model,nof_list[n])\n",
    "        X_rfe = rfe.fit_transform(X_arr,y_arr)\n",
    "        cv_score_list = cross_val_score(model, X_rfe, \n",
    "                                        y_arr, cv=5, scoring='neg_mean_squared_error')\n",
    "        \n",
    "        if(cv_score_list.mean() < neg_score):\n",
    "            neg_score = cv_score_list.mean()\n",
    "            nof = nof_list[n]\n",
    "            \n",
    "    ## getting list of optimum features\n",
    "    \n",
    "    model = LinearRegression()\n",
    "\n",
    "    # initializing RFE model\n",
    "    rfe = RFE(model, nof)    \n",
    "\n",
    "    # transforming data using RFE\n",
    "    X_rfe = rfe.fit_transform(X_arr,y_arr)\n",
    "\n",
    "    # fitting the data to model\n",
    "    model.fit(X_rfe, y_arr)\n",
    "    \n",
    "    # saving model to file\n",
    "    pkl_filename = \"linear_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    temp = pd.Series(rfe.support_, index = cols)\n",
    "    selected_features_rfe = temp[temp==True].index\n",
    "\n",
    "    print(\"Optimum number of features: %d\" %nof)\n",
    "    print(\"Selected features:\", selected_features_rfe)\n",
    "    print(\"Score with %d features: %f\" % (nof, neg_score))\n",
    "    \n",
    "    return neg_score\n",
    "\n",
    "\n",
    "# building the model\n",
    "final_score = linear_model(X_arr, y_arr, cols)\n",
    "\n",
    "# saving tuned model score to master dict\n",
    "model_scores[\"Linear RFE\"] = final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection via Filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = df_merged.corr()\n",
    "sns.heatmap(cor, annot=False, cmap=plt.cm.Reds)\n",
    "plt.show()\n",
    "\n",
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"value\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[(cor_target>0.2) & (cor_target != 1.0)]  # anything above 0.2 gives nothing\n",
    "relevant_features\n",
    "\n",
    "# removing features with high levels of multicollinearity\n",
    "selected_features_corr = df_merged[list(relevant_features.index)].corr()\n",
    "\n",
    "# identifying all the features that have a correlation higher than 0.90 or lower than -0.90 indicating a strong positive or negative correlation\n",
    "\n",
    "threshold_1 = 0.90\n",
    "threshold_2 = -0.90\n",
    "\n",
    "def features_high_corr(df_features_corr):\n",
    "    columns = np.full((df_features_corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(df_features_corr.shape[0]):\n",
    "        for j in range(i+1, df_features_corr.shape[0]):\n",
    "            if (df_features_corr.iloc[i,j] >= threshold_1) | (df_features_corr.iloc[i,j] <= threshold_2) :\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    selected_columns = df_features_corr.columns[columns]\n",
    "    return selected_columns\n",
    "\n",
    "# list of features that are not highly correlated\n",
    "selected_features = features_high_corr(selected_features_corr)\n",
    "print(\"Features in dataset that are not highly correlated: \")\n",
    "print(selected_features)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
