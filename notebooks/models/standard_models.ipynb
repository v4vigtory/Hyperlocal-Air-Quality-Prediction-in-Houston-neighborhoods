{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e8b5a3",
   "metadata": {},
   "source": [
    "# Hyperlocal Air Quality Prediction\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The [Environmental Defense Fund (EDF)](https://www.edf.org/airqualitymaps) has partnered with Google Earth Outreach to map air pollution at the **hyperlocal level**. In various cities around the world, mobile air quality sensors were used to gather air pollution data at street level. This new approach allows researchers to collect much more data at a granular level and showed how air poluution varied over very short distances. \n",
    "\n",
    "Our project focuses on the data collected in [Houston](https://www.edf.org/airqualitymaps/houston), where low cost mobile sensors were outfitted on city fleet vehicles and Google Street View cars, in addition to gathering data from stationary sensors. For this report, we have focused on only one type of pollution - NO2 emissions.\n",
    "\n",
    "Based on our review of literature and exploring various datasets, we have decided the scope of our project to explore the use of machine learning models to predict air quality at any location in Houston based on meteorlogical conditions, sources of emissions from various facilities, and traffic.\n",
    "\n",
    "### Inspiration\n",
    "\n",
    "Our project is inspired and informed by Varsha Gopalakrishnan's \"Hyperlocal Air Quality Prediction using Machine Learning\" project, outlined on [Medium](https://towardsdatascience.com/hyperlocal-air-quality-prediction-using-machine-learning-ed3a661b9a71). Gopalakrishnan performed her analysis using Oakland data, and we intend to replicate most aspects of the approach for Houston.\n",
    "\n",
    "### Data\n",
    "\n",
    "The level of pollution at any place depends on a number of factors like traffic on major streets; emissions from different facilities like railroads, ports, and industries; and meteorological factors like temperature and precipitation. Our target variable is Air Quality Data obtained from the aforementioned pilot done by EDF. We conducted exploratory data analysis for the air quality dataset in our [Houston air quality data Jupyter Notebook](https://github.com/vjoseph21/air-quality-prediction/blob/main/notebooks/cleaning/air_quality.ipynb).\n",
    "\n",
    "We classified our potential features into the following three buckets:\n",
    "\n",
    "#### 1. Traffic Data - [Link to notebook](https://github.com/vjoseph21/air-quality-prediction/blob/main/notebooks/cleaning/traffic_data.ipynb)\n",
    "We look at the geographical range for which EDF collected the air quality data in Houston. The coordinates for the area around Houston is based on minimum and maximum of latitude and longitude from the EDF data. We chose this area as the bounding box to determine location of all traffic signals / intersections within that boundary.\n",
    "\n",
    "The Overpass API from Open Street Maps is used to determine the location of all traffic signals (intersections) within a given bounding box. The Overpy library is used to send the request to the API and this call returns the latitude and longitude of all traffic signals. Next, the distance between each traffic intersection and each point in the monitoring data is measured. A traffic score is calculated as the 'Number of traffic intersections within 1,000 ft of each point in the monitoring data.\"\n",
    "\n",
    "Going forward, in addition to intersections, we will also extend the analysis to look at the distance of bus stops and highways from the NO2 monitoring points.\n",
    "\n",
    "#### 2. Emissions Data - [Link to notebook](https://github.com/vjoseph21/air-quality-prediction/blob/main/notebooks/cleaning/facility_data.ipynb)\n",
    "\n",
    "We use NEI's data for point sources - which provides facility specific pollutant information from across the US. Similar to how we bound based on a boundary box region, we consider only those facilities that fall within the (lat, long) range for which EDF data was collected.\n",
    "\n",
    "After grouping some of the sources based on their types and range of emission values, we calculate the distance between the location of every pollutant data read (from the EDF data) to each facility. We also calculate the emissions per distance measure for each such row, with the idea being that both the quantity of NO2 emission from the facility, as well as the distance from the facility will affect NO2 concentration at a certain point\n",
    "\n",
    "Going forward, we will invest time on three fronts:\n",
    "1. Understanding sources currently marked as 'unknown', since they could add vital information (and variance) to the dataset\n",
    "2. Carrying out a more robuts outlier treatment of the NO2 distribution\n",
    "3. Calculating additional aggregated numeric metrics such as number of point sources (per source_group) in a given radius from the monitoring point\n",
    "\n",
    "#### 3. Meteorological factors - [Link to notebook](https://github.com/vjoseph21/air-quality-prediction/blob/main/notebooks/cleaning/met_data.ipynb)\n",
    "Following Gopalakrishnan's approach, we use the [Daymet](https://daymet.ornl.gov/web_services) API to collect our meteorological data. Daymet's \"daily surface weather and climatological summaries\" provide daily data at a 1km grid granularity, extrapolated from less-granular meteorological observations. Through approximately 200 calls to the Daymet API, we gathered the available daily data (maximum temperature, minimum temperature, shortwave radiation, vapor pressure, and precipitation) at latitudes and longitudes corresponding to the locations of our air quality measurements, and produced averages for each metric across the 9 months when EDF was collecting air quality data in Houston.\n",
    "\n",
    "Going forward, we plan to explore alternate approaches to collapsing the daily data for each air quality measurement site - for example, it's possible that minimums or maximums over the 9-month period in question would be more meaningful than averages.\n",
    "\n",
    "### Modeling \n",
    "We will use this dataset to develop a model to predict air quality in neighbourhoods in Houston where the EDF project did not collect air quality data. We plan to first train the machine learning model to predict NO2 emissions on the same locations as the EDF data and then use the trained model and additional traffic, features, and emissions data to preduct concentrations elsewhere in Houston.\n",
    "\n",
    "For the baseline model, we have merged all the features mentioned above into one dataset and used linear regression for feature selection. We decided to use a threshold of $\\pm90$% to indicate very high correlation between features and dropped them to avoid multicollineairty. We then use the remaining features to run a simple linear regression to guage the siginificance of each feature on NO2 emissions. Our results indicate that minimum temperature, precipitation, and number of intersections are the most important features to predict air quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebad87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import basic python packages for data analysis and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.lines as mlines\n",
    "import pylab as plot\n",
    "import matplotlib\n",
    "import random\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import math\n",
    "import time\n",
    "\n",
    "### Import Scipy stats packages\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "\n",
    "# Import statsmodel packages\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrices\n",
    "\n",
    "# import sklearn packages\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set(style = 'whitegrid')\n",
    "sns.set_palette('bright')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bdbd82",
   "metadata": {},
   "source": [
    "## (A) Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d52ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all datasets to create master_df\n",
    "\n",
    "root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "df_aq = pd.read_csv(root + \"/data/cleaned/air_quality_NO2.csv\", index_col=0)[['value','latitude', 'longitude']]\n",
    "df_met = pd.read_csv(root + \"/data/cleaned/nO2_met.csv\", index_col=0)\n",
    "df_fac = pd.read_csv(root + \"/data/cleaned/no2_fac_data.csv\", index_col=0)\n",
    "# df_fac.drop(df_fac.columns[df_fac.columns.str.contains('_emsdist')], axis=1, inplace=True)\n",
    "df_traffic = pd.read_csv(root + \"/data/cleaned/intersection_final.csv\", index_col=0)\n",
    "\n",
    "df_m1 = df_aq.merge(df_met, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_m2 = df_m1.merge(df_fac, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_merged = df_m2.merge(df_traffic, on = ['latitude', 'longitude'], how = 'inner')\n",
    "df_merged.drop(columns = ['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "X = df_merged.drop(\"value\",1) \n",
    "y = df_merged[\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc62d9",
   "metadata": {},
   "source": [
    "## (B) Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6334d0e4",
   "metadata": {},
   "source": [
    "## (C) Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b10bcc",
   "metadata": {},
   "source": [
    "### (i) Filter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = df_merged.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b09858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"value\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[(cor_target>0.2) & (cor_target != 1.0)]  # anything above 0.2 gives nothing\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf7693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing features with high levels of multicollinearity\n",
    "selected_features_corr = df_merged[list(relevant_features.index)].corr()\n",
    "\n",
    "# identifying all the features that have a correlation higher than 0.90 or lower than -0.90 indicating a strong positive or negative correlation\n",
    "\n",
    "threshold_1 = 0.90\n",
    "threshold_2 = -0.90\n",
    "\n",
    "def features_high_corr(df_features_corr):\n",
    "    columns = np.full((df_features_corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(df_features_corr.shape[0]):\n",
    "        for j in range(i+1, df_features_corr.shape[0]):\n",
    "            if (df_features_corr.iloc[i,j] >= threshold_1) | (df_features_corr.iloc[i,j] <= threshold_2) :\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    selected_columns = df_features_corr.columns[columns]\n",
    "    return selected_columns\n",
    "\n",
    "# list of features that are not highly correlated\n",
    "selected_features = features_high_corr(selected_features_corr)\n",
    "print(\"Features in dataset that are not highly correlated: \")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb939e2",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "The filter method might not be the most accurate way to go about feature selection. None of the met variables are a part of the selected features despite lowering the threshold to 0.2 for Pearson corr coeff. Things to try:\n",
    "1) Modify Pearson corr threshold\n",
    "2) Try other univariate methods apart from Pearson\n",
    "3) Build a suitable model using these features\n",
    "4) Compare with other methods below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b898e3",
   "metadata": {},
   "source": [
    "### (ii) Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f26ef2",
   "metadata": {},
   "source": [
    "#### (a) Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe59a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "reg = LassoCV(cv=cv) #tune this -- when supplied a range of potential alphas, best alpha is 0\n",
    "reg.fit(X, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(reg.coef_, index = X.columns)\n",
    "\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \n",
    "      \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "imp_coef = coef.sort_values()\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f465f8",
   "metadata": {},
   "source": [
    "#### (b) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c56aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af0dcc33",
   "metadata": {},
   "source": [
    "### (iii) Wrapper methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59033fda",
   "metadata": {},
   "source": [
    "#### (a) Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining optimum no of features\n",
    "nof_list=np.arange(1,len(X.columns))            \n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ebc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting list of optimum features\n",
    "\n",
    "cols = list(X.columns)\n",
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, nof)    \n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)\n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)\n",
    "\n",
    "temp = pd.Series(rfe.support_,index = cols)\n",
    "selected_features_rfe = temp[temp==True].index\n",
    "print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787e620",
   "metadata": {},
   "source": [
    "### To-do:\n",
    "1) Incorporate CV in RFE \\\n",
    "2) Try with different models \\\n",
    "3) Why do we fit the model on all data in second step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a0ad8",
   "metadata": {},
   "source": [
    "### (iv) Dimensionality Reduction - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef24d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72873392",
   "metadata": {},
   "source": [
    "## (D) Modeling - Using Cross Validation and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB -- Code to get list of params for an estimator:\n",
    "#regressor.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106be632",
   "metadata": {},
   "source": [
    "### (1) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490eead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training set (there is no pre-prepared test set for CV)\n",
    "X_train = X.iloc[:, :].values\n",
    "y_train = y.iloc[:].values\n",
    "\n",
    "# scaling the data\n",
    "feature_scaler = StandardScaler()\n",
    "X_train = feature_scaler.fit_transform(X_train)\n",
    "\n",
    "# defining the regressor\n",
    "regressor = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# using grid search -- use more hyperparameters\n",
    "grid_param = {\n",
    "    'n_estimators': [100, 300, 500, 800, 1000],\n",
    "    'criterion': ['mse'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=regressor,\n",
    "                     param_grid=grid_param,\n",
    "                     scoring='neg_mean_squared_error',\n",
    "                     cv=5,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "gd_sr.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = gd_sr.best_params_\n",
    "print(best_parameters)\n",
    "\n",
    "best_result = gd_sr.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee2d2c8",
   "metadata": {},
   "source": [
    "### (2) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ee5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training set (there is no pre-prepared test set for CV)\n",
    "X_train = X.iloc[:200, :3].values\n",
    "y_train = y.iloc[:200].values\n",
    "\n",
    "# scaling the data\n",
    "feature_scaler = StandardScaler()\n",
    "X_train = feature_scaler.fit_transform(X_train)\n",
    "\n",
    "# defining the regressor\n",
    "regressor = XGBRegressor(random_state=0)\n",
    "\n",
    "# using grid search -- use more hyperparameters\n",
    "grid_param = {\n",
    "        'maxdepth':[8],\n",
    "        'objective':['multi:softmax'],\n",
    "        'n_estimators':[600, 900, 1200],\n",
    "        'gamma':[0, .1, .2],\n",
    "        'lambda':[.5, 1, 3],\n",
    "        'alpha':[.5, 1, 2],\n",
    "        'num_class':[3]\n",
    "        }\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=regressor,\n",
    "                     param_grid=grid_param,\n",
    "                     scoring='neg_mean_squared_error',\n",
    "                     cv=2,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "gd_sr.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = gd_sr.best_params_\n",
    "print(best_parameters)\n",
    "\n",
    "best_result = gd_sr.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be4abf",
   "metadata": {},
   "source": [
    "### (3) Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training set (there is no pre-prepared test set for CV)\n",
    "X_train = X.iloc[:200, :3].values\n",
    "y_train = y.iloc[:200].values\n",
    "\n",
    "# scaling the data\n",
    "feature_scaler = StandardScaler()\n",
    "X_train = feature_scaler.fit_transform(X_train)\n",
    "\n",
    "# defining the regressor\n",
    "regressor = Lasso(random_state=0)\n",
    "\n",
    "# using grid search -- use more hyperparameters\n",
    "grid_param = {\n",
    "        'alpha':[1e-3, 1e-2, 1e-1, 1]\n",
    "        }\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=regressor,\n",
    "                     param_grid=grid_param,\n",
    "                     scoring='neg_mean_squared_error',\n",
    "                     cv=2,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "gd_sr.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = gd_sr.best_params_\n",
    "print(best_parameters)\n",
    "\n",
    "best_result = gd_sr.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d4e09",
   "metadata": {},
   "source": [
    "### (4) Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "896e33dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.001}\n",
      "-1.943138985224071e-07\n"
     ]
    }
   ],
   "source": [
    "# creating training set (there is no pre-prepared test set for CV)\n",
    "X_train = X.values\n",
    "y_train = y.values\n",
    "\n",
    "# scaling the data\n",
    "feature_scaler = StandardScaler()\n",
    "X_train = feature_scaler.fit_transform(X_train)\n",
    "\n",
    "# defining the regressor\n",
    "regressor = Ridge(random_state=0)\n",
    "\n",
    "# using grid search -- use more hyperparameters\n",
    "grid_param = {\n",
    "        'alpha':[1e-3, 1e-2, 1e-1, 1]\n",
    "        }\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=regressor,\n",
    "                     param_grid=grid_param,\n",
    "                     scoring='neg_mean_squared_error',\n",
    "                     cv=5,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "gd_sr.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = gd_sr.best_params_\n",
    "print(best_parameters)\n",
    "\n",
    "best_result = gd_sr.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f3bce9",
   "metadata": {},
   "source": [
    "## (E) Model evaluation / Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073ac69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
