{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading and Cleaning Meterological Data** <br>\n",
    "Carolyn Vilter <br>\n",
    "<br>\n",
    "Inspiration / Meteorological API approach from Varsha Gopalakrishnan<br>\n",
    "https://github.com/varsha2509/hyperlocal-aq-prediction/blob/master/Notebooks/Daymet-Data-API-Call.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import daymetpy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Vishal's air quality sensor data\n",
    "# Air quality sensors will be the rows/obs of the final dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daymet_avg(df1, year1, year2):\n",
    "    df1.reset_index(inplace=True) #Reset index such that we have a column as YYYY-MM-DD\n",
    "    df1.rename(columns={'index':'Datestamp'}, inplace=True) #Rename index as Datestamp\n",
    "    start_date = str(year1) + '-06-01' #Set start date\n",
    "    end_date = str(year2) + '-05-31'   #Set end data\n",
    "    df1 = df1[(df1['Datestamp']>= start_date) & (df1['Datestamp'] <= end_date)] # Filter for days only between June 2015 to May 2016\n",
    "    df1.drop(columns=['swe'], inplace = True) #Drop snow water equivalent since its always zero in Oakland\n",
    "    df1_avg = np.mean(df1) #Calculate the average value of all measurements in a given location\n",
    "    return (df1_avg['dayl'],df1_avg['prcp'],df1_avg['srad'], df1_avg['tmax'] ,df1_avg['tmin'] ,df1_avg['vp'])\n",
    "daily_met_average_data = [] \n",
    "start_year = 2015 \n",
    "end_year = 2016 \n",
    "for row in df_measured.head(10.iterrows(): \n",
    "    lon, lat = (row[1]['Longitude'], row[1]['Latitude']) \n",
    "    daymet_raw_df = make_api_call(lat, lon, start_year, end_year) \n",
    "    daily_met_average_data.append(daymet_avg(daymet_raw_df, start_year, end_year)) ## create a list with daymet parameters for each location\n",
    "compute_daily_met_average_df = pd.DataFrame(daily_met_average_data) #convert list to a dataframe\n",
    "daymet_average = daymet_average.join(df_measured)\n",
    "daymet_average.to_csv(\"Data/daymet_avg_15_16.csv\")\n",
    "\n",
    "####\n",
    "\n",
    "denver_loc = (-104.9903, 39.7392)\n",
    "miami_loc = (-80.2089, 25.7753)\n",
    "\n",
    "denver = daymetpy.daymet_timeseries(lon=denver_loc[0], lat=denver_loc[1], start_year=2012, end_year=2014)\n",
    "miami = daymetpy.daymet_timeseries(lon=miami_loc[0], lat=miami_loc[1], start_year=2012, end_year=2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "houston_loc = (-95.3698, 29.7604)\n",
    "houston = daymetpy.daymet_timeseries(lon=houston_loc[0], lat=houston_loc[1], start_year=2017, end_year=2018)\n",
    "houston"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
